{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oVm-m7-58qN"
   },
   "source": [
    "## Лабораторная работа №5 (Поиск по векторной БД)\n",
    "\n",
    "\n",
    "Необходимо записать ваш датасет в векторную базу данных и выполнить эксперименты по поиску схожих фрагментов текста, соответствующих запросу.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Jx4lFD8q_Wy"
   },
   "source": [
    "### Import & set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall llmx==0.0.15a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lida==0.0.10\n",
    "!pip install llmx==0.0.15a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install evaluate\n",
    "!pip install llama-cpp-python\n",
    "!pip install pinecone-client\n",
    "!pip install langchain==0.0.300\n",
    "!pip install chromadb==0.4.12\n",
    "!pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pinecone\n",
    "import glob\n",
    "import os\n",
    "import chromadb\n",
    "import nltk\n",
    "from langchain.document_loaders import PDFMinerLoader, TextLoader, CSVLoader, UnstructuredWordDocumentLoader, UnstructuredHTMLLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGI1qS2wNu2c"
   },
   "source": [
    "## Show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/MyDrive/nlp-news/train.csv', header=None, names = ['category','title', 'text'])\n",
    "df['ID'] = df.index\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U4j6ZjhyAxe"
   },
   "source": [
    "## Разбиение текстовых документов на фрагменты.\n",
    "\n",
    "> Разработать алгоритм разбиения текстовых документов на фрагменты текста. Можно использовать уже существующие механизмы, например, разбиение по длине фрагмента текста в символах и пересечению с соседними фрагментами. Дополнительные баллы за усложненные варианты, например: учитывать границы токенов и предложений.\n",
    "\n",
    "> Подготовка метаданных для каждого фрагмента, таких как класс документа, автор документа, ключевые слова и др."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def cut_text_by_sent(text, fragment_len, overlay):\n",
    "    sentences = sent_tokenize(text)\n",
    "    fragments = []\n",
    "    current_fragment = []\n",
    "    current_len = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        if current_len + len(sent) <= fragment_len:\n",
    "            current_fragment.append(sent)\n",
    "            current_len += len(sent)\n",
    "        else:\n",
    "            if current_fragment:\n",
    "                fragments.append(' '.join(current_fragment))\n",
    "            current_fragment = [sent]\n",
    "            current_len = len(sent)\n",
    "\n",
    "    final_fragments = []\n",
    "\n",
    "    for fragment in fragments:\n",
    "        if len(fragment) >= fragment_len:\n",
    "            all_len = 0\n",
    "            len_text = len(fragment)\n",
    "            while all_len + fragment_len <= len_text:\n",
    "                final_fragments.append(fragment[all_len:all_len + fragment_len])\n",
    "                all_len += overlay\n",
    "        else:\n",
    "            final_fragments.append(fragment)\n",
    "\n",
    "    return final_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = 'Магистрант, совмещающий обучение с трудовой деятельностью, вправе проходить практику по месту трудовой деятельности, если профессиональная деятельность, осуществляемая им, соответствует требованиям к содержанию практики. Магистрант должен своевременно подать заявление (см. Приложение 2) с заверенной в организации копией трудового договора и согласие на обработку персональных данных (см. Приложение 3) куратору практики. Куратор практики подтверждает соответствие профессиональной деятельности требованиям к практике или отклоняет место практики из-за несоответствия профессиональной деятельности требованиям к практике.'\n",
    "text_fragments = cut_text_by_sent(long_text, 200, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Text sample: ', long_text)\n",
    "print('Result:', text_fragments)\n",
    "print('Size: ', [len(f) for f in text_fragments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3RPY08gt3U5"
   },
   "source": [
    "## Векторизация фрагментов текста\n",
    "\n",
    "Векторизация фрагментов текста. В качестве метода векторизации можно использовать стороний api (huggingface, openai, etc.), w2v или любую другую модель на выбор. Рекомендуется применить модель с huggingface. Подходящие модели с huggingface можно выбрать по ссылке.\n",
    "\n",
    "Модель: paraphrase-multilingual-mpnet-base-v2\n",
    "\n",
    "Создание Векторной Базы Данных (ВБД).\n",
    "\n",
    "Необходимо выбрать одну из доступных ВБД, например: Chroma (рекомендуемая с точки зрения простоты), Pinecone и т.д.\n",
    "\n",
    "Реализовать механизм загрузки и сохранения текстовых данных в ВБД."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "\n",
    "  def load_single_document(self, file_path: str):\n",
    "    pass\n",
    "\n",
    "  def load_documents(self, source_dir: str):\n",
    "    pass\n",
    "\n",
    "class Embedder():\n",
    "  # vectorization via SentenceTransformer\n",
    "  def __init__(self):\n",
    "    self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "  # vect for sentense\n",
    "  def get_embeddings(self, sentences):\n",
    "    return [[float(e) for e in list(emb)] for emb in list(self.model.encode(sentences))]\n",
    "\n",
    "class ChromaDB():\n",
    "  # ChromaDB Client Init\n",
    "  def __init__(self):\n",
    "\n",
    "    # self.client = chromadb.Client()\n",
    "    # Disk connect\n",
    "    self.client = chromadb.PersistentClient(path=\"/content/gdrive/MyDrive/nlp-news\")\n",
    "\n",
    "  def clear(self, name):\n",
    "    self.client.delete_collection(name=name)\n",
    "    return self.client.list_collections()\n",
    "\n",
    "  def get_collection(self, name):\n",
    "    return self.client.get_collection(name=name)\n",
    "\n",
    "  def get_collections(self):\n",
    "    return self.client.list_collections()\n",
    "\n",
    "class ChromaCollection():\n",
    "  def __init__(self, collection_name, similarity, client):\n",
    "    self.collection_name = collection_name\n",
    "    self.similarity = similarity\n",
    "    self.client = client\n",
    "    self.collection = self.client.get_or_create_collection(name=collection_name, metadata={\"hnsw:space\": similarity})\n",
    "\n",
    "  def add(self, embeddings, texts, topics, ids):\n",
    "    self.collection.add(\n",
    "        embeddings = embeddings,\n",
    "         documents = texts,\n",
    "         metadatas = [{\"source\": \"df\", \"category\":f\"{topic}\"} for i, topic in enumerate(topics)],\n",
    "         ids = [f'id {i}' for i in ids]\n",
    ")\n",
    "\n",
    "  # get number of results simular data via embeddings\n",
    "  def query(self, embeddings, n_results):\n",
    "    return self.collection.query(\n",
    "      query_embeddings=embeddings,\n",
    "       n_results=n_results,\n",
    "    )\n",
    "\n",
    "  # get all doc\n",
    "  def get(self):\n",
    "    return self.collection.get()\n",
    "\n",
    "  # count doc in collection\n",
    "  def count(self):\n",
    "    return self.collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = embedder.get_embeddings(df['text'][:30000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChromaDB()\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCq1Dyq1IJv1"
   },
   "source": [
    "## Поиск схожих фрагментов текста\n",
    "Выбрать алгоритмы similarity для поиска схожих фрагментов текста.\n",
    "Реализовать механизм поиска схожих фрагментов по заданным запросам.\n",
    "\n",
    "* **Косинусное сходство (Cosine Similarity)**: Этот алгоритм измеряет косинус угла между двумя векторами, представляющими текстовые фрагменты. Более высокое значение косинусного сходства указывает на более близкое сходство между фрагментами.\n",
    "* **Евклидово расстояние (Euclidean Distance)**: Этот алгоритм измеряет расстояние между двумя точками в n-мерном пространстве. Для текстовых фрагментов, которые представлены как точки в пространстве, меньшее значение евклидова расстояния указывает на более близкое сходство.\n",
    "* **IP-расстояние (Integral Projection Distance)**: Этот алгоритм измеряет сходство между двумя распределениями, основываясь на их форме и значении проекций. Для этого оно вычисляет площадь между интегральными проекциями двух распределений. Чем меньше площадь между проекциями, тем больше схожесть между распределениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cos_sim', 'l2_sim' и 'Ip_sim' - имена коллекций, которые будут созданы в БД ChromaDB\n",
    "# 'cosine', 'l2' и 'ip' - типы схожести (similarity) для каждой коллекции.\n",
    "# cosine - косинусное расстояние\n",
    "# l2 - евклидово расстояние\n",
    "# ip - произведение скалярного умножения\n",
    "collection_cos = ChromaCollection('cos_sim', 'cosine', client.client)\n",
    "collection_l2 = ChromaCollection('l2_sim', 'l2', client.client)\n",
    "collection_Ip = ChromaCollection('Ip_sim', 'ip', client.client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds - векторные представления документов, которые нужно добавить в коллекцию\n",
    "# texts - тексты документов, которые нужно добавить в коллекцию\n",
    "# topics - темы (метаданные) документов, которые нужно добавить в коллекцию\n",
    "# ids - идентификаторы документов, которые нужно добавить в коллекцию\n",
    "collection_cos.add(embeds[0:30000], df['text'].values.tolist()[0:30000], df['category'].values.tolist()[0:30000], df['ID'].values.tolist()[0:30000])\n",
    "collection_l2.add(embeds[0:30000], df['text'].values.tolist()[0:30000], df['category'].values.tolist()[0:30000], df['ID'].values.tolist()[0:30000])\n",
    "collection_Ip.add(embeds[0:30000], df['text'].values.tolist()[0:30000], df['category'].values.tolist()[0:30000], df['ID'].values.tolist()[0:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num of samples\n",
    "print(collection_cos.count())\n",
    "print(collection_l2.count())\n",
    "print(collection_Ip.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LvANw2dSFDa"
   },
   "source": [
    "## Оценка качества поиска\n",
    "\n",
    "Сгенерировать набор запросов к ВБД.\n",
    "\n",
    "Провести оценку качества поиска, определяя, насколько хорошо схожие фрагменты отображаются в результатах поиска. Оценку можно выполнить следующими способами:\n",
    "на основе ручной оценки качества запросов и соответствующих ответов;\n",
    "посчитать средний порядковый номер требуемого фрагмента в отсортированном по релевантности спике результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test samples\n",
    "news = [\n",
    "    \"Unknown Nesterenko Makes World Headlines (Reuters). Reuters - Belarus Yuliya Nesterenko won the top\\women's athletics gold medal at the Olympics on Saturday,\\triumphing over a field stripped of many big names because of\\doping woes to win the 100 meters.\",\n",
    "    \"Producer sues for Rings profits Hollywood producer Saul Zaentz sues the producers of The Lord of the Rings for \\$20m in royalties.\",\n",
    "    \"South Korean police used water cannon in central Seoul Sunday to disperse at least 7,000 protesters urging the government to reverse a controversial decision to\\send more troops to Iraq.\",\n",
    "    \"Russia ready to contribute to settlement of South Ossetia conflict: Putin. MOSCOW, Aug. 18 (Xinhuanet) -- Russian President Vladimir Putin said Wednesday that Russia is ready to contribute to a settlement of conflict between Georgia and its separatist province of South Ossetia.\",\n",
    "    \"Hobbit-finding Boffins in science top 10. AP - Australian scientists who helped discover a species of tiny humans nicknamed Hobbits have been hailed for making the second most important scientific achievement of 2004\",\n",
    "    \"Kevin Hartman made seven saves for Los Angeles, and Jon Busch had two saves for Columbus as the Galaxy and Crew played to a 0-0 tie Saturday night.\",\n",
    "    \"Exploring Andromeda (SPACE.com) SPACE.com - Although winter officially begins on Dec. 21 at 7:40 a.m. EST, \\  one of the landmarks of the autumn sky is still readily visible, high toward \\  the south around 7 p.m. local time.\",\n",
    "    \"Pricey Drug Trials Turn Up Few New Blockbusters The \\$500 billion drug industry is stumbling badly in its core business of finding new medicines, while aggressively marketing existing drugs.\",\n",
    "    \"PRESS START FOR NOSTALGIA Like Led Zeppelin #39;s  #39; #39;Stairway to Heaven #39; #39; and Lynyrd Skynyrd #39;s  #39; #39;Freebird, #39; #39; classic video games like Frogger and Pong can bring back an entire era.\",\n",
    "    \"Russia shrugs off US court freeze on oil giant Yukos auction MOSCOW (AFP) - Russia forged ahead with the weekend auction of the core asset of crippled oil giant Yukos despite a disputed US court order barring the sale, with state-controlled gas giant Gazprom entering the bidding.\",\n",
    "    \"NASA #39;s departing chief, Sean O #39;Keefe, on Friday defended his decision to pursue a robotic repair mission to the Hubble Space Telescope, days after a panel of scientists said a shuttle mission would be better.\",\n",
    "    \"Cisco invests \\$12 million in Japan R amp;D center On Thursday, the company announced it will invest \\$12 million over the next five years in a new research and development center in Tokyo.\",\n",
    "    \"Michael Phelps took care of qualifying for the Olympic 200-meter freestyle semifinals Sunday, and then found out he had been added to the American team for the evening's 400 freestyle relay final. Phelps' rivals Ian Thorpe and Pieter van den Hoogenband and teammate Klete Keller were faster than the teenager in the 200 free preliminaries.\",\n",
    "    \"Whitman: EBay To Buy Rent.com; Compliments Craigslist Stake Without reserve. EBay (nasdaq: EBAY - news - people ) on Friday said it is buying Rent.com. The latter, which is privately held, provides online listings of apartment and house rentals\",\n",
    "    \"Sporadic gunfire and shelling took place overnight in the disputed Georgian region of South Ossetia in violation of a fragile ceasefire, wounding seven Georgian servicemen.\",\n",
    "    \"Robinho #39;s mother releaased by kidnappers The mother of Santos striker Robinho was released unharmed on Friday, 40 days after she was kidnapped at a family gathering. Marina da Silva de Souza, 44, appeared healthy but thinner than when she was abducted\"\n",
    "    ]\n",
    "\n",
    "#Related questions\n",
    "questions = [\n",
    "    \"What has Yuliya Nesterenko won?\",\n",
    "    \"Which movie's producers are being sued by Saul Zaentz?\",\n",
    "    \"Why did South Korean police use water cannon in central Seoul?\",\n",
    "    \"What is Russia ready to do with South Ossetia conflict?\",\n",
    "    \"What was the second most important scientific achievement of 2004?\",\n",
    "    \"How did Kevin Hartman and Jon Busch influence the outcome of the match?\",\n",
    "    \"What time and date the winter officially begins?\",\n",
    "    \"How much billion the drug industry is lost?\",\n",
    "    \"Which games can bring back an entire era?\",\n",
    "    \"Who is court freeze on oil giant Yukos auction with state-controlled gas giant Gazprom entering the bidding?\",\n",
    "    \"Who was the Departing Chief of NASA?\",\n",
    "    \"In which city is a development center that Cisco invests 12 million?\",\n",
    "    \"Who did take care of qualifying for the Olympic 200-meter freestyle semifinals?\",\n",
    "    \"What is Rent.com that EBay bought?\",\n",
    "    \"What happened in the disputed Georgian region of South Ossetia overnight, violating the fragile ceasefire and causing injuries to Georgian servicemen?\",\n",
    "    \"How many days have past since she was kidnapped at a family gathering?\"\n",
    "    ]\n",
    "\n",
    "\n",
    "answers = [\n",
    "    \"The top\\women's athletics gold medal\",\n",
    "    \"The Lord of the Rings\",\n",
    "    \"To disperse at least 7,000 protesters urging the government to reverse a controversial decision to\\send more troops to Iraq.\",\n",
    "    \"Ready to contribute to settlement of South Ossetia conflict\",\n",
    "    \"Discover a species of tiny humans nicknamed Hobbits\",\n",
    "    \"They made saves for their respective teams, resulting in a 0-0 tie\",\n",
    "    \"Winter officially begins on Dec. 21 at 7:40 a.m.\",\n",
    "    \"The $500 billion drug industry is stumbling badly\",\n",
    "    \"Classic video games like Frogger and Pong\",\n",
    "    \"US\",\n",
    "    \"Sean O #39;Keefe\",\n",
    "    \"Tokyo\",\n",
    "    \"Michael Phelps\",\n",
    "    \"Provides online listings of apartment and house rentals\",\n",
    "    \"Sporadic gunfire and shelling took place, resulting in injuries to seven Georgian servicemen\",\n",
    "    \"40 days\"\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embeds = embedder.get_embeddings(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ожидаемое максимальное кол-во результатов поиска до 1000\n",
    "results_cos = collection_cos.query(q_embeds,1000)\n",
    "print('Base news ',  news[0])\n",
    "print('Result\\n ', results_cos['documents'][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_l2 = collection_l2.query(q_embeds,1000)\n",
    "print('Base news ',  news[0])\n",
    "print('Result\\n ', results_l2['documents'][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ip = collection_Ip.query(q_embeds,1000)\n",
    "print('Base news ',  news[0])\n",
    "print('Result\\n ', results_ip['documents'][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2C1AvPb6L8J"
   },
   "source": [
    "## Лабораторная работа №6 (Question Answering)\n",
    "\n",
    "Необходимо запустить и протестировать QA на основе LLM модели, можно выбрать любую LLM модель (рекомендуется искать на huggingface):\n",
    "\n",
    "\n",
    "> на основе llama.cpp - можно запустить на GPU и/или на CPU, примеры:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация вопросно-ответной модели roberta-base-squad2\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embeds = embedder.get_embeddings(questions)\n",
    "results = collection_cos.query(q_embeds,5)\n",
    "print('Base news ',  news[0])\n",
    "print('Student answer ',  answers[0])\n",
    "print('Result\\n ', results['documents'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install bert_score\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_all = []\n",
    "for q, a, index in zip(questions, answers, range(len(answers))):\n",
    "  QA_input = {'question': q,\n",
    "             'context': ' '.join(results['documents'][index])}\n",
    "  res = nlp(QA_input)\n",
    "  bs = bertscore.compute(predictions=[res['answer']], references=[a], lang=\"en\")\n",
    "  bs_all.append(bs)\n",
    "\n",
    "  print(f'Question: {q}\\nAnswer: {res[\"answer\"]}\\nUser answer: {a}\\nScore: {bs[\"f1\"][0]}\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# среднее значение метрики по всем вопросам\n",
    "f1_scores = [bs['f1'][0] for bs in bs_all]\n",
    "sum(f1_scores)/len(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5adSr-b6vxV"
   },
   "source": [
    "# **Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio==3.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QA call\n",
    "def echo(question, history):\n",
    "    q_embeds = embedder.get_embeddings([question])\n",
    "    results = collection_cos.query(q_embeds,5)\n",
    "    QA_input = {'question': question,\n",
    "             'context': ' '.join(results['documents'][0])}\n",
    "    res = nlp(QA_input)\n",
    "    return res['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# графический интерфейс\n",
    "demo = gr.ChatInterface(fn=echo, examples=[\"hello\", \"hola\", \"merhaba\"], title=\"QA Bot\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
