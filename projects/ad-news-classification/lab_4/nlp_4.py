# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12zzgR_MAWYGf3lVeSj5QoXwjQknlqzcB
"""

import os

import numpy as np
import time

from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import torch
import pytorch_lightning as pl

print("Script started", flush=True)

assets_path = ".."
train_embeddings = os.path.realpath(assets_path + "/assets/annotated-corpus/train_embeddings.tsv")
test_embeddings = os.path.realpath(assets_path + "/assets/annotated-corpus/test_embeddings.tsv")
topics = os.listdir(os.path.realpath(assets_path + "/assets/annotated-corpus/train"))


def prepare_dataset(filename):
    x_raw = []
    y_raw = []
    with open(filename) as f:
        lines = f.readlines()[:-1]
        for line in lines:
            y_raw.append(topics.index(line.split("\t", 1)[0].split("\\")[0]))
            x_raw.append(list(map(float, line.split("\t", 1)[1].split("\t"))))

    return np.array(x_raw), np.array(y_raw)


print("Prepare train", flush=True)
x_train, y_train = prepare_dataset(train_embeddings)
print("Prepare test", flush=True)
x_test, y_test = prepare_dataset(test_embeddings)

clf1 = SVC()
print("Train SVC", flush=True)
clf1.fit(x_train, y_train)
print("Train finished", flush=True)
preds = clf1.predict(x_test)


def calculate_raw_metrics(pred, gt):
    metrics_dict = {}
    if pred.shape != gt.shape:
        raise RuntimeError("Shapes doesn't fit")
    for i in gt:
        if i not in metrics_dict.keys():
            metrics_dict[i] = {"tp": 0, "fp": 0, "tn": 0, "fn": 0}
    for i in range(pred.shape[0]):
        if pred[i] == gt[i]:
            metrics_dict[gt[i]]["tp"] += 1
            for j in metrics_dict.keys():
                if j != gt[i]:
                    metrics_dict[j]["tn"] += 1
        else:
            metrics_dict[pred[i]]["fp"] += 1
            metrics_dict[gt[i]]["fn"] += 1

    return metrics_dict


def calculate_metrics(pred, gt):
    raw_dict = calculate_raw_metrics(pred, gt)
    metrics_dict = {}
    global_tp = 0
    global_fp = 0
    global_tn = 0
    global_fn = 0
    metrics_dict["precision_macro"] = 0
    metrics_dict["recall_macro"] = 0
    metrics_dict["f1_macro"] = 0
    for i in raw_dict.keys():
        metrics_dict[i] = {}
        metrics_dict[i]["precision"] = raw_dict[i]["tp"] / (raw_dict[i]["tp"] + raw_dict[i]["fp"])
        metrics_dict[i]["recall"] = raw_dict[i]["tp"] / (raw_dict[i]["tp"] + raw_dict[i]["fn"])
        metrics_dict[i]["f1"] = 2 * raw_dict[i]["tp"] / (2 * raw_dict[i]["tp"] + raw_dict[i]["fp"] + raw_dict[i]["fn"])
        global_tp += raw_dict[i]["tp"]
        global_tn += raw_dict[i]["tn"]
        global_fn += raw_dict[i]["fn"]
        global_fp += raw_dict[i]["fp"]
        metrics_dict["precision_macro"] += metrics_dict[i]["precision"] / len(raw_dict.keys())
        metrics_dict["recall_macro"] += metrics_dict[i]["recall"] / len(raw_dict.keys())
        metrics_dict["f1_macro"] += metrics_dict[i]["f1"] / len(raw_dict.keys())
    metrics_dict["precision_micro"] = global_tp / (global_tp + global_fp)
    metrics_dict["recall_micro"] = global_tp / (global_tp + global_fn)
    metrics_dict["f1_micro"] = 2 * global_tp / (2 * global_tp + global_fn + global_fp)
    metrics_dict["accuracy"] = global_tp / gt.shape[0]
    return metrics_dict


def reference_metrics(pred, gt):
    print("Precision:", precision_score(gt, pred, average=None), flush=True)
    print("Recall:", recall_score(gt, pred, average=None), flush=True)
    print("F1:", f1_score(gt, pred, average=None), flush=True)
    print("Precision macro:", precision_score(gt, pred, average="macro"), flush=True)
    print("Recall macro:", recall_score(gt, pred, average="macro"), flush=True)
    print("F1 macro:", f1_score(gt, pred, average="macro"), flush=True)
    print("Precision micro:", precision_score(gt, pred, average="micro"), flush=True)
    print("Recall micro:", recall_score(gt, pred, average="micro"), flush=True)
    print("F1 micro:", f1_score(gt, pred, average="micro"), flush=True)
    print("Accuracy:", accuracy_score(gt, pred), flush=True)


print("___SVC___", flush=True)
print(calculate_metrics(preds, y_test), flush=True)

reference_metrics(preds, y_test)


def train_evaluate_model(model, x_train, y_train, x_test, y_test, model_name):
    print(f"Start train {model_name}", flush=True)
    t_begin = time.time()
    model.fit(x_train, y_train, )
    t_end = time.time()
    preds = model.predict(x_test)
    metrics = calculate_metrics(preds, y_test)
    print(f"___{model_name}___", flush=True)
    print("Precision macro:", metrics["precision_macro"], flush=True)
    print("Recall macro:", metrics["recall_macro"], flush=True)
    print("F1 macro:", metrics["f1_macro"], flush=True)
    print("Precision micro:", metrics["precision_micro"], flush=True)
    print("Recall micro:", metrics["recall_micro"], flush=True)
    print("F1 micro:", metrics["f1_micro"], flush=True)
    print("Accuracy:", metrics["accuracy"], flush=True)
    print("Time:", t_end - t_begin, flush=True)
    return model


svm_linear = train_evaluate_model(SVC(kernel="linear"), x_train, y_train, x_test, y_test, "SVC_linear")

svm_poly = train_evaluate_model(SVC(kernel="poly"), x_train, y_train, x_test, y_test, "SVC_poly")

svm_rbf = train_evaluate_model(SVC(kernel="rbf"), x_train, y_train, x_test, y_test, "SVC_rbf")

svm_sigmoid = train_evaluate_model(SVC(kernel="sigmoid"), x_train, y_train, x_test, y_test, "SVC_sigmoid")




class MLP(pl.LightningModule):

    def __init__(self):
        super().__init__()

        # Building a linear encoder
        self.layers = torch.nn.Sequential(
            torch.nn.Linear(100, 512),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 1024),
            torch.nn.ReLU(),
            torch.nn.Linear(1024, 2048),
            torch.nn.ReLU(),
            torch.nn.Linear(2048, 2048),
            torch.nn.ReLU(),
            torch.nn.Linear(2048, 1024),
            torch.nn.ReLU(),
            torch.nn.Linear(1024, 512),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 4),
            torch.nn.Softmax(dim=0)
        )
        self.loss_func = torch.nn.CrossEntropyLoss()

    def forward(self, x):
        return self.layers(x)

    def training_step(self, batch, batch_idx):
        x = batch[:, :100]
        y = batch[:, 100:]
        y_hat = self.layers(x)
        loss = self.loss_func(y_hat, y)

        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x = batch[:, :100]
        y = batch[:, 100:]
        y_hat = self.layers(x)
        loss = self.loss_func(y_hat, y)
        self.log("val_loss", loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(),
                                     lr=1e-5,
                                     weight_decay=1e-8)
        return optimizer


y_train_onehot = torch.nn.functional.one_hot(torch.tensor(y_train, dtype=torch.int64))
data = torch.cat((torch.tensor(x_train, dtype=torch.float64), y_train_onehot), dim=1).float()
y_test_onehot = torch.nn.functional.one_hot(torch.tensor(y_test, dtype=torch.int64), num_classes = 4).float()
data_test = torch.cat((torch.tensor(x_test, dtype=torch.float32), y_test_onehot), dim=1)


train_loader = torch.utils.data.DataLoader(dataset=data, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=data_test, batch_size=32, shuffle=False)
early_stop_callback = pl.callbacks.EarlyStopping(monitor="val_loss")
checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor="val_loss")
logger = pl.loggers.TensorBoardLogger("tensorboard_logs/")
trainer = pl.Trainer(max_epochs=200, logger=logger)

mlp = MLP()
trainer.fit(mlp, train_loader, test_loader)

preds = torch.argmax(mlp.forward(torch.tensor(x_test).float()).detach(), dim=1).numpy()

print("___PERCEPTRON___", flush=True)
print(calculate_metrics(preds, y_test))

perm = np.random.permutation(len(x_train))

train_evaluate_model(SVC(kernel="rbf"), x_train[perm][:9000], y_train[perm][:9000], x_test, y_test, "SVC_rbf_slice_9k")

train_evaluate_model(SVC(kernel="rbf"), x_train[perm][:6000], y_train[perm][:6000], x_test, y_test, "SVC_rbf_slice_6k")

train_evaluate_model(SVC(kernel="rbf"), x_train[perm][:2000], y_train[perm][:2000], x_test, y_test, "SVC_rbf_slice_2k")

train_evaluate_model(SVC(kernel="rbf"), x_train[:, 10:90], y_train, x_test[:, 10:90], y_test, "SVC_rbf_slice_80d")

from sklearn.decomposition import PCA

pca = PCA(n_components=50)
pca.fit(x_train)

train_evaluate_model(SVC(kernel="rbf"), pca.transform(x_train), y_train, pca.transform(x_test), y_test,
                     "SVC_rbf_pca_50")

pca_2 = PCA(n_components=2)

draw_data = pca_2.fit_transform(x_train[perm][:500])

np.save("draw_data.npy", draw_data)

x_train_extended = np.concatenate((x_train, np.sin(x_train), np.cos(x_train)), axis=1)
x_test_extended = np.concatenate((x_test, np.sin(x_test), np.cos(x_test)), axis=1)

train_evaluate_model(SVC(kernel="rbf"), x_train_extended, y_train, x_test_extended, y_test, "SVC_rbf_extended")
