{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b26fb355c7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420cfb06c1f3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_dir = os.path.realpath(\"../assets/annotated-corpus\")\n",
    "train_dir = os.path.join(assets_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbffc9c44c7843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e482236537c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for t in topics:\n",
    "    workdir = os.path.join(train_dir, t)\n",
    "    for filename in os.listdir(workdir):\n",
    "        with open(os.path.join(workdir, filename)) as f:\n",
    "            lines = \"\".join(f.readlines())\n",
    "            sentences_raw = lines.split(\"\\n\\n\")\n",
    "            for s in sentences_raw:\n",
    "                words = s.split(\"\\n\")\n",
    "                if len(words) == 0 or words[0] == \"\":\n",
    "                    continue\n",
    "                stems_raw = list(map(lambda x: x.split(\"\\t\")[1], words))\n",
    "                lemmas = list(map(lambda x: x.split(\"\\t\")[2], words))\n",
    "                stems = []\n",
    "                for i in range(len(stems_raw)):\n",
    "                    if lemmas[i] not in stopwords.words(\"english\"):\n",
    "                        stems.append(stems_raw[i])\n",
    "                sentences.append(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee431edefba4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b2390f64e8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8232d82f82f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = []\n",
    "word_count = {}\n",
    "ngrams_count = {}\n",
    "for s in sentences:\n",
    "    counter = 0\n",
    "    for w in s:\n",
    "        if w not in word_count.keys():\n",
    "            word_count[w] = 0\n",
    "        word_count[w] += 1\n",
    "        counter += 1\n",
    "    if counter < ngram_length:\n",
    "        continue\n",
    "    for i in range(len(s) - ngram_length + 1):\n",
    "        ngram = tuple(s[i:i+ngram_length])\n",
    "        if ngram not in ngrams_count.keys():\n",
    "            ngrams_count[ngram] = 0\n",
    "        ngrams_count[ngram] += 1\n",
    "        ngrams.append(ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d2bf792d524e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439c66a95bedb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_count.items(), key=lambda x: -x[1])[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccccee88e69fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ngrams_count.items(), key=lambda x: -x[1])[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57340fb3a953ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(word_count.values())\n",
    "total_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a676f2b41e06348",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### f(n, c) - ngrams_count[ngram], частота встречаемости ключевого слова n в паре с коллокатом c;\n",
    "### N - total_words, общее число словоупотреблений в корпусе (тексте);\n",
    "### П_i f(u_i) - count_mul_result, Произведение абсолютных частот i-й униграммы в 3-грамме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239eade116446b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_score = {}\n",
    "for ngram in set(ngrams):\n",
    "    count_mul_result = 1\n",
    "    for word in ngram:\n",
    "        count_mul_result *= word_count[word]\n",
    "    ngram_score[ngram] = np.log2(ngrams_count[ngram] * (total_words**(ngram_length-1)) / count_mul_result)\n",
    "sorted(ngram_score.items(), key=lambda x: -x[1])[0:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ceaa1ed22338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_score = {}\n",
    "for ngram in set(ngrams):\n",
    "    count_sum_log_result = 0\n",
    "    for word in ngram:\n",
    "        count_sum_log_result += np.log2(word_count[word])\n",
    "    ngram_score[ngram] = np.log2(ngrams_count[ngram]) + (ngram_length-1)*np.log2(total_words) - count_sum_log_result\n",
    "sorted(ngram_score.items(), key=lambda x: -x[1])[30:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7f548f73d5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import  *\n",
    "from nltk import Text\n",
    "import nltk\n",
    "text = []\n",
    "for s in sentences:\n",
    "    text += s\n",
    "finder = TrigramCollocationFinder.from_words(Text(text))\n",
    "finder.nbest(nltk.collocations.TrigramAssocMeasures().mi_like, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544e7480a9c22b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
