{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv_file(path_file):\n",
    "    sentences = []\n",
    "    with open(path_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                word = line.split('\\t')[0].strip()\n",
    "                if word:\n",
    "                    sentences.append(word)\n",
    "    return sentences\n",
    "\n",
    "def build_token_dictionary(sentences):\n",
    "    token_freq = Counter(sentences)\n",
    "    token_freq = {token: freq for token, freq in token_freq.items() if freq > 0 and token not in stop_words and re.sub(r\"[^\\w\\s]\", \"\", token) != ''}\n",
    "    return token_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder  = '/media/space/ssd_1_tb_evo_sumsung/MishaHW/train'\n",
    "folder_category = os.listdir(path_folder)\n",
    "\n",
    "documents = []\n",
    "sentences = []\n",
    "\n",
    "for folder in tqdm(folder_category):\n",
    "    include_files = os.listdir(os.path.join(path_folder, folder))\n",
    "    for file in include_files:\n",
    "        path_file = f'{path_folder}/{folder}/{file}'\n",
    "        sentence = read_tsv_file(path_file)\n",
    "        sentences += sentence\n",
    "        documents.append(f'{folder}/{file}')\n",
    "\n",
    "token_freq = build_token_dictionary(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('token_freq.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(token_freq, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "terms = list(token_freq.keys())\n",
    "\n",
    "def create_term_document_matrix(documents, terms):\n",
    "    matrix = {'Word': terms}\n",
    "    for doc in tqdm(documents):\n",
    "        path_file = os.path.join(path_folder, doc)\n",
    "        sentence = read_tsv_file(path_file)\n",
    "        tk_freq = build_token_dictionary(sentence)\n",
    "        count = []\n",
    "        for token in terms:\n",
    "            if token in list(tk_freq.keys()):\n",
    "                count.append(tk_freq[token])\n",
    "            else:\n",
    "                count.append(0)\n",
    "        matrix[doc] = count\n",
    "    df = pd.DataFrame(matrix)\n",
    "    df.to_csv('output-lab2.csv', index=False)\n",
    "    return pd.DataFrame(matrix)\n",
    "\n",
    "term_document_matrix = create_term_document_matrix(documents, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/media/space/ssd_1_tb_evo_sumsung/MishaHW/output-lab2.csv')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    matches = []\n",
    "    cleaned_tokens = []\n",
    "    regex_patterns = {\n",
    "        \"phone_numbers\": r'(?:\\+\\d{1,3}\\s)?(?:\\(\\d{3}\\)|\\d{1,3})\\s?-?\\s?\\d{3,4}\\s?-?\\s?\\d{3,4}',\n",
    "        \"emails\": r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        \"emoji\": r'[:;8BX=][-\\'`^]?[)D(|/\\\\]+|<3|/\\(\\s*\\d+\\s*[-+*xX]\\s*\\d+\\s*\\)|\\B<3\\b|\\bhearts?\\b',\n",
    "    }\n",
    "    for pattern in regex_patterns.values():\n",
    "        matches.extend(re.findall(pattern, text))\n",
    "    for i, match in enumerate(matches):\n",
    "        text = text.replace(match, f'__regex_match_{i}__')\n",
    "    \n",
    "    new_sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\n)\\s', text)\n",
    "\n",
    "    for sentences in new_sentences:\n",
    "        # print(sentences.split('\\n'))\n",
    "        for line in sentences.split('\\n'):\n",
    "            clear_token = re.findall(r'\\b\\w+\\b|[\\(\\),.—:;!?|<>\"]', line)\n",
    "            if clear_token:\n",
    "                cleaned_tokens.append(clear_token)\n",
    "\n",
    "    for sentence in cleaned_tokens:\n",
    "        for num, token in enumerate(sentence):\n",
    "            for i, match in enumerate(matches):\n",
    "                if token == f'__regex_match_{i}__':\n",
    "                    sentence[num] = match\n",
    "    return cleaned_tokens\n",
    "\n",
    "def remove_punct_marks(data):\n",
    "   clear_data = list()\n",
    "   for sentence in data:\n",
    "      sentence_data = [re.sub(r\"[^\\w\\s]\",\"\", lemma) for lemma in sentence if re.sub(r\"[^\\w\\s]\",\"\", lemma.lower()) and lemma.lower() not in stop_words]\n",
    "      clear_data.append(sentence_data)\n",
    "   return clear_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_texts_with_word(df, word):\n",
    "    word_data = df[df[df.columns[0]] == word]\n",
    "    count = (word_data.iloc[0, 1:] > 0).sum()\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def vectorize_tf_idf(text, df):\n",
    "    tokens = text\n",
    "    tf = {}\n",
    "    idf = {}\n",
    "    tf_idf = {}\n",
    "    result = []\n",
    "    clear_tokens = tokens\n",
    "    new_clear_tokens = []\n",
    "    token_fr = build_token_dictionary(clear_tokens)\n",
    "\n",
    "    total_words = sum(token_fr.values())\n",
    "    documents_count = len(df.columns[1:])\n",
    "\n",
    "\n",
    "    for word in list(df['Word']):\n",
    "        if word in token_fr.keys():\n",
    "            tf[word] = token_fr[word] / total_words\n",
    "            idf[word] = math.log((documents_count / count_texts_with_word(df, word)) + 1)\n",
    "            tf_idf[word] = tf[word] * idf[word]\n",
    "            result.append(tf[word] * idf[word])\n",
    "        else:\n",
    "            tf_idf[word] = 0.0\n",
    "            result.append(0.0)\n",
    "    return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the quick brown jumped over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vectorize_tf_idf(text, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "path_folder  = '/media/space/ssd_1_tb_evo_sumsung/MishaHW/train'\n",
    "folder_category = os.listdir(path_folder)\n",
    "\n",
    "train_sentences = []\n",
    "\n",
    "for folder in tqdm(folder_category):\n",
    "    include_files = os.listdir(os.path.join(path_folder, folder))\n",
    "    for file in include_files:\n",
    "        path_file = f'{path_folder}/{folder}/{file}'\n",
    "        sentence = read_tsv_file(path_file)\n",
    "        train_sentences.append(sentence)\n",
    "\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar(\"computer\")\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_custom(vec1, vec2):\n",
    "    \"\"\"Вычисляет косинусное сходство между двумя векторами.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "cosine_similarity_custom(model.wv[\"woman\"], model.wv[\"system\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "sim_tokens = ['dog', 'woman', 'man', 'person', 'white', 'black', 'green', 'yellow', 'computer', 'system', 'machine', 'program']\n",
    "vectorized_sim = [model.wv[token] for token in sim_tokens]\n",
    "pca = PCA(n_components=2)\n",
    "res = pca.fit_transform(vectorized_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sim_tokens[1:]:\n",
    "    print(f'Current token: dog -- Selected token: {token} --->{cosine_similarity_custom(model.wv[\"dog\"], model.wv[token])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, token in enumerate(sim_tokens):\n",
    "    plt.scatter(res[i, 0], res[i, 1])\n",
    "    plt.text(res[i, 0]+0.01, res[i, 1]+0.01, token, fontsize=9)\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('2D')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_folder  = '/media/space/ssd_1_tb_evo_sumsung/MishaHW/20news-bydate-train'\n",
    "# folder_category = os.listdir(path_folder)\n",
    "\n",
    "term_text = []\n",
    "\n",
    "for sample_content in tqdm(list(df['Word'])):\n",
    "    tokens = vectorize_tf_idf(str(sample_content),df)\n",
    "    term_text.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=len(model.wv[0]))\n",
    "terms_transformed = pca.fit_transform(term_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_compare = pd.DataFrame(terms_transformed)\n",
    "terms_to_compare.index = df['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(word1, word2):\n",
    "    print(f\"Words: {word1} --- {word2}\")\n",
    "    print(\"W2Vec:\", cosine_similarity_custom(model.wv[word1], model.wv[word2]))\n",
    "    print(\"Tf-Idf:\", cosine_similarity_custom(terms_to_compare.loc[word1], terms_to_compare.loc[word2]))\n",
    "\n",
    "compare('dog', 'cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare('god', 'hell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare('god', 'angels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare('woman', 'system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, model):\n",
    "    res_text = [0 for i in range(model.vector_size)]\n",
    "    sentences = tokenize_text(text)\n",
    "    # sentences = remove_punct_marks(sentences)\n",
    "    for sentence in sentences:\n",
    "        res_sent = [0 for i in range(model.vector_size)]\n",
    "        for token in sentence:\n",
    "            if model.wv.has_index_for(token):\n",
    "                res_sent += model.wv[token]\n",
    "        res_sent = np.array(res_sent) / len(res_sent)\n",
    "        res_text += res_sent\n",
    "    return res_text /  len(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the quick brown jumped over the lazy dog. Gaf, dasds.\"\n",
    "vectorize_text(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_foldes = '/media/space/ssd_1_tb_evo_sumsung/MishaHW/20news-bydate-train'\n",
    "\n",
    "catalogs = os.listdir(path_foldes)\n",
    "for catalog in catalogs:\n",
    "    include_catalog = os.listdir(os.path.join(path_foldes, catalog))\n",
    "    for file in include_catalog:\n",
    "        if os.path.isdir(os.path.join(path_foldes, f'{catalog}/{file}')):\n",
    "            catalogs.append(f'{catalog}/{file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tsv = {}\n",
    "for catalog in tqdm(catalogs):\n",
    "    include_catalog = os.listdir(os.path.join(path_foldes, catalog))\n",
    "    for file in include_catalog:\n",
    "        if not os.path.isdir(os.path.join(path_foldes, f'{catalog}/{file}')):\n",
    "            with open(os.path.join(path_foldes, f'{catalog}/{file}'), 'r', encoding='latin1') as file_name:\n",
    "                sample_content = file_name.read()\n",
    "            vect = vectorize_text(sample_content, model)\n",
    "            dict_tsv[f'{catalog}/{file}'] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/space/ssd_1_tb_evo_sumsung/MishaHW/train_embeddings.tsv\", \"w\") as f:\n",
    "    for k in dict_tsv.keys():\n",
    "        print(k.replace(\".tsv\", \"\"), *dict_tsv[k], sep=\"\\t\", file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
