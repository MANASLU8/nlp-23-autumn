{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv_file(path_file):\n",
    "   sentences = list()\n",
    "   sentence = list()\n",
    "   with open(path_file) as file:\n",
    "     for line in file:\n",
    "         if line != \"\\n\":\n",
    "            lemma = line.split('\\t')[2]\n",
    "            sentence.append(lemma[:-1]) \n",
    "         if line == \"\\n\" and sentence:\n",
    "           sentences.append(sentence)\n",
    "           sentence = list()\n",
    "   return sentences\n",
    "\n",
    "\n",
    "def remove_punct_marks(data):\n",
    "   clear_data = list()\n",
    "   for sentence in data:\n",
    "      sentence_data = [re.sub(r\"[^\\P{P}-]+\",\"\", lemma.lower()) for lemma in sentence if re.sub(r\"[^\\P{P}-]+\",\"\", lemma.lower()) and lemma.lower() not in stop_words]\n",
    "      clear_data.append(sentence_data)\n",
    "   return clear_data\n",
    "\n",
    "\n",
    "def extract_trigrams(clear_data):\n",
    "    trigrams = list()\n",
    "    for sentence in clear_data:\n",
    "        for i in range(len(sentence) - 2):\n",
    "            trigrams.append(sentence[i:i+3])\n",
    "    return trigrams\n",
    "\n",
    "\n",
    "def count_words(clear_data):\n",
    "    f_words = dict()\n",
    "    for sentence in clear_data:\n",
    "        for word in sentence:  \n",
    "            if word not in f_words:\n",
    "                f_words[word] = 1\n",
    "            else:\n",
    "                f_words[word] += 1\n",
    "    return f_words\n",
    "\n",
    "\n",
    "def t_score(trigram_frequencies, word_counts, total_words):\n",
    "    t_scores = {}\n",
    "    for trigram, count in trigram_frequencies.items():\n",
    "        expected_frequency = (word_counts[trigram[0]] / total_words) * \\\n",
    "                            (word_counts[trigram[1]] / total_words) * \\\n",
    "                            (word_counts[trigram[2]] / total_words) * total_words\n",
    "        t_score = (count - expected_frequency) / math.sqrt(count)\n",
    "        t_scores[trigram] = t_score\n",
    "    return t_scores\n",
    "\n",
    "\n",
    "def trigram_frequencies(trigrams_data):\n",
    "    trigram_frequencie = dict()\n",
    "    for trigram in trigrams_data:\n",
    "        if tuple(trigram) not in trigram_frequencie:\n",
    "            trigram_frequencie[tuple(trigram)] = 1\n",
    "        else:\n",
    "            trigram_frequencie[tuple(trigram)] += 1\n",
    "    return trigram_frequencie\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = read_tsv_file('/media/space/ssd_1_tb_evo_sumsung/MishaHW/train/alt.atheism/49960.tsv')\n",
    "clear_data = remove_punct_marks(data)\n",
    "\n",
    "trigrams_data = extract_trigrams(clear_data)\n",
    "word_counts = count_words(clear_data)\n",
    "\n",
    "trigram_frequencie = trigram_frequencies(trigrams_data)\n",
    "total_words = sum(word_counts.values())\n",
    "\n",
    "t_scores = t_score(trigram_frequencie, word_counts, total_words)\n",
    "\n",
    "sorted_t_scores = sorted(t_scores.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "for trigram, score in sorted_t_scores:\n",
    "    print(f\"{trigram}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "f = open('20news-bydate-train/alt.atheism/49960')\n",
    "raw = f.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(raw,'english',True)\n",
    "print(tokens[:10])\n",
    "\n",
    "# text = nltk.Text(tokens)\n",
    "\n",
    "# finder_thr = TrigramCollocationFinder.from_words(text)\n",
    "\n",
    "# print(finder_thr.nbest(trigram_measures.pmi, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "flat_list = [item for sublist in clear_data for item in sublist]\n",
    "finder_thr = TrigramCollocationFinder.from_words(flat_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(finder_thr.nbest(trigram_measures.student_t, 30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
