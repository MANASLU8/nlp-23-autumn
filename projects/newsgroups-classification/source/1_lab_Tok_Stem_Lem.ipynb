{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_text(text):\n",
    "    matches = []\n",
    "    cleaned_tokens = []\n",
    "    regex_patterns = {\n",
    "        \"phone_numbers\": r'(?:\\+\\d{1,3}\\s)?(?:\\(\\d{3}\\)|\\d{1,3})\\s?-?\\s?\\d{3,4}\\s?-?\\s?\\d{3,4}',\n",
    "        \"emails\": r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        \"emoji\": r'[:;8BX=][-\\'`^]?[)D(|/\\\\]+|<3|/\\(\\s*\\d+\\s*[-+*xX]\\s*\\d+\\s*\\)|\\B<3\\b|\\bhearts?\\b',\n",
    "    }\n",
    "    for pattern in regex_patterns.values():\n",
    "        matches.extend(re.findall(pattern, text))\n",
    "    for i, match in enumerate(matches):\n",
    "        text = text.replace(match, f'__regex_match_{i}__')\n",
    "    \n",
    "    new_sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\n)\\s', text)\n",
    "\n",
    "    for sentences in new_sentences:\n",
    "        # print(sentences.split('\\n'))\n",
    "        for line in sentences.split('\\n'):\n",
    "            clear_token = re.findall(r'\\b\\w+\\b|[\\(\\),.—:;!?|<>\"]', line)\n",
    "            if clear_token:\n",
    "                cleaned_tokens.append(clear_token)\n",
    "\n",
    "    for sentence in cleaned_tokens:\n",
    "        for num, token in enumerate(sentence):\n",
    "            for i, match in enumerate(matches):\n",
    "                if token == f'__regex_match_{i}__':\n",
    "                    sentence[num] = match\n",
    "    return cleaned_tokens\n",
    "\n",
    "# def processing_sentences(sentences):\n",
    "#     tokens = [tokenize_text(sentence) for sentence in sentences]\n",
    "#     return tokens\n",
    "\n",
    "# tokken = [\"Москва — столица Российской Федерации. Привет :) мир\\n 89205198722 som@nncs.ru, kfk\", \"Saveley loh. Privet!!\"]\n",
    "# print(processing_sentences(tokken))\n",
    "# processing_sentences(tokken)\n",
    "# sample_file_path = '/media/space/ssd_1_tb_evo_sumsung/MishaHW/20news-bydate-test/alt.atheism/53068'\n",
    "# with open(sample_file_path, 'r', encoding='latin1') as file_name:\n",
    "#     sample_content = file_name.read()\n",
    "# print(tokenize_text(sample_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_tokens(tokens_list):\n",
    "    stemmed_tokens_list = list()\n",
    "    for tokens in tokens_list:\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        stemmed_tokens_list.append(stemmed_tokens)\n",
    "    return stemmed_tokens_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_tokens(tokens_list):\n",
    "    lemmatized_tokens_list = list()\n",
    "    for tokens in tokens_list:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "        lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "    return lemmatized_tokens_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tsv_file(tokens, stemmed_words, lemmatized_words, output_path, output_filename):\n",
    "    output = f'{output_path}/{output_filename}.tsv'\n",
    "    with open(output, 'w', encoding='utf-8') as output_file:\n",
    "        for token_sentence, stemmed_sentence, lemmatized_sentence in zip(tokens, stemmed_words, lemmatized_words):\n",
    "            for token, stemma, lemma in zip(token_sentence, stemmed_sentence, lemmatized_sentence):\n",
    "                  output_file.write(f\"{token}\\t{stemma}\\t{lemma}\\n\")\n",
    "            output_file.write(f\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def processing_files(category_path, file, category_path_new):\n",
    "        sample_file_path = os.path.join(category_path, file)\n",
    "        with open(sample_file_path, 'r', encoding='latin1') as file_name:\n",
    "            sample_content = file_name.read()\n",
    "        tokens = tokenize_text(sample_content)\n",
    "        lemmatized_words = lemmatized_tokens(tokens)\n",
    "        stemmed_words = stemmed_tokens(tokens)\n",
    "\n",
    "        save_tsv_file(tokens, stemmed_words, lemmatized_words, category_path_new, file)\n",
    "\n",
    "def processing_other_folders(category_path, folder, file_, folder_new):\n",
    "    files_category = os.listdir(category_path)\n",
    "    folder_new = os.path.join(folder_new, file_)\n",
    "    os.makedirs(f'{folder_new}')\n",
    "    for file in tqdm(files_category, desc=f'Folder: {folder} | Class: {file_}'):\n",
    "        if os.path.isdir(os.path.join(category_path, file)):\n",
    "            print(os.path.join(category_path, file))\n",
    "            processing_other_folders(os.path.join(category_path, file), folder, file, folder_new)\n",
    "        if os.path.isfile(os.path.join(category_path, file)):\n",
    "            processing_files(category_path, file, folder_new)\n",
    "\n",
    "def processing_main_folders(folder, folder_new):\n",
    "    folder_category = os.listdir(folder)\n",
    "    for category in folder_category:\n",
    "        category_path = os.path.join(folder, category)\n",
    "        category_path_new = os.path.join(folder_new, category)\n",
    "        os.makedirs(f'{category_path_new}')\n",
    "        files_category = os.listdir(category_path)\n",
    "        for file in tqdm(files_category, desc=f'Folder: {folder} | Class: {category}'):\n",
    "            if os.path.isfile(os.path.join(category_path, file)):\n",
    "                processing_files(category_path, file, category_path_new)\n",
    "            elif os.path.isdir(os.path.join(category_path, file)):\n",
    "                processing_other_folders(os.path.join(category_path, file), folder, file, category_path_new)\n",
    "\n",
    "\n",
    "def processing(folders):\n",
    "    for folder in folders:\n",
    "        folder_new = folder.split('-')[-1]\n",
    "        processing_main_folders(folder, folder_new)\n",
    "        \n",
    "\n",
    "folders = ['20news-bydate-test', '20news-bydate-train']\n",
    "processing(folders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
