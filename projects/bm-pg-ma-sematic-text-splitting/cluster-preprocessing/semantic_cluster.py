# -*- coding: utf-8 -*-
"""Semantic cluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15hwBzXMehKBcaHdIFXc5ddJ7BdobzaH_

#Кластеризация текстов для составления контекста

"""

"""Читаем датасет и собираем его в текст"""

import os
import pandas as pd
from sklearn.cluster import KMeans
import numpy as np
import re
from sentence_transformers import SentenceTransformer
import chromadb


def split_to_sent(text):
    sentences = re.split(
        r"(((?<!\w\.\w.)(?<!\s\w\.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s(?=[A-Z]))|((?<![\,\-\:])\n(?=[A-Z]|\" )))", text)[
                ::4]
    return sentences


text_dict = {1: None,
             2: None,
             3: None,
             4: None}


def read_str_to_np(x):
    n_x = x.replace('\n', '').replace("[", '').replace(']', '').split(' ')
    return np.array([float(f) for f in n_x if f])


print("Start reading", flush=True)
for l in text_dict.keys():
    load_path = os.path.join('/datasets', 'train_' + str(l) + '.csv')
    df = pd.read_csv(load_path)
    df['cosine'] = df.apply(lambda x: read_str_to_np(x['cosine']), axis=1)
    text_dict[l] = df

"""Для каждого лэйбла делаем кластеризатор

Кластеризуем, проставляем лэйблы кластера для каждой записи

Все записи с одним лэйблом кластера склеиваем в один текст
"""


def fill_claster(df):
    df['claster'] = df.apply(lambda x: kmeans.predict(x['cosine'].reshape(1, -1))[0], axis=1)
    return df


print("K-means", flush=True)

k_dict = {}
for l in text_dict.keys():
    print(f"K-means for label: {l}", flush=True)
    news_art_emb = np.stack(text_dict[l]['cosine'])
    kmeans = KMeans(init='k-means++', n_clusters=500, random_state=0, n_init="auto").fit(news_art_emb)
    k_dict[l] = kmeans
    fill_claster(text_dict[l])


def fill_cent(df, centers):
    d = []
    for index, row in df.iterrows():
        d.append(centers[row['claster']])
    df['cent'] = d


for l in text_dict.keys():
    df = text_dict[l]
    cent = k_dict[l].cluster_centers_
    fill_cent(df, cent)

"""##В качестве алгоритма будем использовать К-means. Он позволит дополнять кластеры малого объема ближайшими (по расстоянию центроидов) кластерами

Соберем тексты одного лэйбла в один текст, сохраняя информацию о центроидах
"""


def get_text_by_claster(df):
    text_by_clst = {}
    for cl in df['claster'].unique():
        df_cl = df[df['claster'] == cl]
        text = ''
        src_rows = []
        d = {}
        for index, row in df_cl.iterrows():
            text = text + '\n\n' + row['text']
            src_rows.append(row['Unnamed: 0'])
            d['cent'] = row['cent']
        d['text'] = text
        d['src_rows'] = src_rows

        text_by_clst[cl] = d
    return text_by_clst


text_by_label_dict = {}
for l in text_dict.keys():
    text_by_label_dict[l] = get_text_by_claster(text_dict[l])

"""Для каждого получившегося текста посчитаем длину"""

for l in text_by_label_dict.keys():
    t_b_cl = text_by_label_dict[l]
    for cl in t_b_cl.keys():
        t_b_cl[cl]['len'] = len(t_b_cl[cl]['text'])

"""## Определяем минимальный размер, меньше которого текст нужно склеивать с соседним кластером"""

N = 2000

from scipy.spatial.distance import cosine


def get_dist(p1, p2):
    return cosine(p1, p2)


def get_neig(cent, s_cl, d):
    min_d = None
    min_cl = None
    for cl in d.keys():
        if s_cl == cl:
            continue
        cur_cent = d[cl]['cent']
        cur_dist = get_dist(cent, cur_cent)
        if min_d is None:
            min_d = cur_dist
            min_cl = cl
            continue
        if cur_dist < min_d:
            min_d = cur_dist
            min_cl = cl
    print(f"Found neigh: {s_cl} and {min_cl}", flush=True)
    return min_cl


def merge_d(d, cl1, cl2):
    print(f"merge {cl2} to {cl1}", flush=True)
    n_cent = (d[cl1]['cent'] + d[cl2]['cent']) / 2
    d[cl1]['text'] = d[cl1]['text'] + '\n' + d[cl2]['text']
    d[cl1]['cent'] = n_cent
    d[cl1]['src_rows'] += d[cl2]['src_rows']
    del d[cl2]


def get_min(d):
    min = None
    min_cl = None
    for k in d.keys():
        if min is None:
            min = len(d[k]['text'])
            min_lc = k
            continue
        cur_len = len(d[k]['text'])
        if cur_len < min:
            min = cur_len
            min_cl = k
    print(f"cl with min len: {min_cl}, len: {len(d[min_cl]['text'])}", flush=True)
    return min_cl


print(f"Merge clusters", flush=True)
cp = dict(text_by_label_dict)
for l in cp.keys():
    print(f'start for label: {l}', flush=True)
    d = cp[l]

    min_cl = get_min(d)
    while len(d[min_cl]['text']) < N:
        min_cl = get_min(d)
        cent = d[min_cl]['cent']

        neigh = get_neig(cent, min_cl, d)
        merge_d(d, neigh, min_cl)

        min_cl = get_min(d)

"""#АЛГОРИТМ ИМЕНИ ГЛЕБА ПРОСКУРИНА"""

model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')


def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def split_to_chunks(text, min, max):
    marked = [0] * text.__len__()
    print(f"Encoding", flush=True)
    embeddings = model.encode(text, device='cuda',
                              # да я лох без gpu и что
                              normalize_embeddings=True)
    print(f"splitting", flush=True)
    res_list = []
    for i in range(len(text)):
        if i % 100 == 0:
            print(f"iter: {i}", flush=True)
        res_list_temp = []
        if marked[i] == 1:
            continue
        marked[i] = 1
        res_list_temp.append(i)
        curr_text_size = len(text[i])
        flag = True
        while flag:
            max_pos = -1
            curr_max = 0
            # 1 2 3 4 5 6
            # 1 0 0 1 0 0
            for j in range(i + 1, len(text)):
                if marked[j] == 1 or len(text[j]) + curr_text_size > max:
                    continue
                if abs(cosine_similarity(embeddings[j], embeddings[i])) > curr_max:
                    max_pos = j
                    curr_max = abs(cosine_similarity(embeddings[j], embeddings[i]))

            if max_pos != -1:
                res_list_temp.append(max_pos)
                marked[max_pos] = 1
                curr_text_size = curr_text_size + len(text[max_pos])
            else:
                if curr_text_size < min:
                    for k in range(len(res_list)):
                        marked[k] = 0
                else:
                    res_list.append(res_list_temp)

                curr_text_size = 0
                flag = False

    return res_list


def concat(list_sent):
    t = ''
    for s in list_sent:
        t += s
    return t


def split_text_to_chunks(sem_split_func, sent_list, n, N):
    chuncks_ids = sem_split_func(sent_list, n, N)
    chuncks = []
    for chunk in chuncks_ids:
        sents = []
        for id in chunk:
            sents.append(sent_list[id])
        chuncks.append(sents)
    return chuncks


print("Start semantic splitting", flush=True)

result_sent = []
for label in text_by_label_dict.keys():
    for cluster in text_by_label_dict[label].keys():
        print(f"Splitting cluster {cluster} for label {label}", flush=True)
        t = text_by_label_dict[label][cluster]["text"]
        sent_list = split_to_sent(t)
        chunks = split_text_to_chunks(split_to_chunks, sent_list, 500, 2000)
        for chunk in chunks:
            result_sent.append(concat(chunk))

print("Start db fitting", flush=True)


class EmbeddingFunction:
    def __init__(self):
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

    def __call__(self, input):
        return self.model.encode(input).tolist()


class DB:
    def __init__(self, alg_name, root_path):
        self.ef = EmbeddingFunction()
        self.client = chromadb.PersistentClient(path=root_path)
        self.distance_function = "cosine"
        self.alg_name = alg_name
        self.collection = self.client.get_or_create_collection("sem_split_" + self.alg_name,
                                                               metadata={"hnsw:space": self.distance_function},
                                                               embedding_function=self.ef)

    def add(self, items):
        old_batch = 0
        new_batch = 1000
        while True:
            if new_batch > len(items["fragments"]):
                break
            self.collection.add(
                documents=items["fragments"][old_batch:new_batch],
                ids=items["ids"][old_batch:new_batch])
            old_batch = new_batch
            new_batch += 1000
        self.collection.add(
            documents=items["fragments"][old_batch:],
            ids=items["ids"][old_batch:])

    def query(self, query, n_results):
        return self.collection.query(query_embeddings=self.ef(query), n_results=n_results)

    def clear(self):
        self.client.delete_collection("sem_split_" + self.alg_name)
        self.collection = self.client.get_or_create_collection("sem_split_" + self.alg_name,
                                                               metadata={"hnsw:space": self.distance_function},
                                                               embedding_function=self.ef)


database_proskurin_mangarakov = DB("proskurin_mangarakov", "/results")

database_proskurin_mangarakov.clear()
database_proskurin_mangarakov.add({"fragments": result_sent, "ids": [str(x) for x in range(len(result_sent))]})
