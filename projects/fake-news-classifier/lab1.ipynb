{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory work #1 (text segmentation and annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset reading is taken from https://www.kaggle.com/code/therealsampat/fake-news-detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('../data/Fake.csv')\n",
    "df_true = pd.read_csv('../data/True.csv')\n",
    "\n",
    "df_fake['class'] = 0\n",
    "df_true['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = df_fake.drop_duplicates('text')\n",
    "df_true = df_true.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.shape, df_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.concat([df_fake, df_true], axis=0)\n",
    "df_merge.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merge.drop(['title', 'subject', 'date'], axis=1)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['part'] = 'train'\n",
    "n = len(df)\n",
    "train_n = int(n * 0.8)\n",
    "val_n = int(n * 0.1)\n",
    "test_n = n - train_n - val_n\n",
    "df.loc[(train_n < df.index) & (df.index < train_n + val_n), 'part'] = 'val'\n",
    "df.loc[train_n + val_n <= df.index, 'part'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['part'] == 'train']), len(df[df['part'] == 'val']), len(df[df['part'] == 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10).to_csv('../data/sample.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if index > 10:\n",
    "        break\n",
    "    print(index, row['text'], row['class'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = df.iloc[df[df['text'] == 'Boos and chants of  Lock her up!  were heard in the crowd assembled at the West Front of the U.S. Capitol Friday morning when defeated Democratic Party presidential nominee Hillary Clinton was introduced at the inaugural ceremony for President-elect Donald Trump.#InaugurationDay Lock her up pic.twitter.com/APVtyyYote  Bill Simms (@Mittens1245) January 20, 2017The crowd on the mall booed when the jumbotron showed a close-up shot of Hillary Clinton at #Inauguration https://t.co/1dvY5lxdKo  gpbnews (@gpbnews) January 20, 2017Some in crowd chanting LOCK HER UP as Hillary Clinton arrives  Jamie Dupree (@jamiedupree) January 20, 2017Via: Gateway Pundit '].index, 0].values[0]\n",
    "print(example_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take care about names (i.e. @jamiedupree should be treated as separate token), hash tags (#Inauguration is one token here). Also let's say that we want to keep web sites as one token (pic.twitter.com/APVtyyYote or https://t.co/1dvY5lxdKo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    # so the website will not split into two separate sentences by comma:\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)(?=\\s|[#])')\n",
    "    sentences = sentence_endings.split(text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "sentences = split_into_sentences(example_text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_words(sentences):\n",
    "    # Regular expression to match URLs, hashtags, handles, words, and standalone punctuation\n",
    "    word_pattern = re.compile(r'pic.twitter.com/\\S+|https?://\\S+|www\\.\\S+|\\#\\S+|\\@\\w+|\\b\\w+\\'?\\w*|[\\w\\'-]+|[.,!?;]')\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_pattern.findall(sentence)\n",
    "        tokenized_sentences.append(words)\n",
    "    return tokenized_sentences\n",
    "\n",
    "tokenized = split_into_words(sentences)\n",
    "for tokens in tokenized:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokenized_sentences, language=\"english\"):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    stemmed_sentences = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        stemmed_sentences.append(stemmed_tokens)\n",
    "    return stemmed_sentences\n",
    "\n",
    "stemmed = stem_words(tokenized)\n",
    "for s in stemmed:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokenized_sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        lemmatized_sentences.append(lemmatized_tokens)\n",
    "    return lemmatized_sentences\n",
    "\n",
    "lemmatized = lemmatize_tokens(tokenized)\n",
    "for l in lemmatized:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    sentences = split_into_sentences(text)\n",
    "    tokenized = split_into_words(sentences)\n",
    "    stemmed = stem_words(tokenized)\n",
    "    lemmatized = lemmatize_tokens(tokenized)\n",
    "    dfs = []\n",
    "    for i in range(len(tokenized)):\n",
    "        data = []\n",
    "        for j in range(len(tokenized[i])):\n",
    "            row = {\n",
    "                'Token': tokenized[i][j],\n",
    "                'Stem': stemmed[i][j],\n",
    "                'Lemma': lemmatized[i][j]\n",
    "            }\n",
    "            data.append(row)\n",
    "        df = pd.DataFrame(data)\n",
    "        dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(df, part):\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row['text']\n",
    "        class_ = 'true' if row['class'] else 'fake'\n",
    "        dir = f'../assets/annotated-corpus/{part}/{class_}'\n",
    "        Path(dir).mkdir(parents=True, exist_ok=True)\n",
    "        path = dir / Path(str(index) + '.tsv')\n",
    "\n",
    "        sentence_dfs = process_text(text)\n",
    "        with open(path, 'w') as f:\n",
    "            for sentence_df in sentence_dfs:\n",
    "                sentence_df.to_csv(f, index=None, sep='\\t', header=None)\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for part in ['train', 'val', 'test']:\n",
    "    write_dataset(df[df['part'] == part], part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
