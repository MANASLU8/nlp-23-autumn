{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory work #3 (text vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np\n",
    "from math import log1p\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(root_dir, n=None):\n",
    "    file_paths = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.tsv'):\n",
    "                file_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    data = []\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if n is not None and i >= n:\n",
    "            break\n",
    "        try:\n",
    "            d = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "            d.columns = ['Token', 'Stem', 'Lemma']\n",
    "        except EmptyDataError as e:\n",
    "            print(i, file_path, e)\n",
    "        data.append(d.dropna())\n",
    "        \n",
    "    \n",
    "    ids = [os.path.splitext(os.path.basename(path))[0] for path in file_paths]\n",
    "    return ids, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, train = read_files('../assets/annotated-corpus/train', \n",
    "                  #  1000\n",
    "                   )\n",
    "# val_ids, val = read_files('../assets/annotated-corpus/val', \n",
    "#                 #  100\n",
    "#                  )\n",
    "test_ids, test = read_files('../assets/annotated-corpus/test', \n",
    "                #   100\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_token(token, token_frequencies, min_frequency=2):\n",
    "    if token in string.punctuation:\n",
    "        return False\n",
    "    if token.lower() in stop_words:\n",
    "        return False\n",
    "    if token_frequencies[token] < min_frequency:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_freqs(dfs):\n",
    "    token_frequencies = Counter()\n",
    "    term_document_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc_id, df in enumerate(dfs):\n",
    "        tokens = df['Token'].tolist()\n",
    "        token_frequencies.update(tokens)\n",
    "\n",
    "        for token in tokens:\n",
    "            if is_valid_token(token, token_frequencies):\n",
    "                term_document_matrix[doc_id][token] += 1\n",
    "                \n",
    "    # filter all tokens that return is_valid_token False\n",
    "    token_frequencies = Counter(dict({(token, freq) for (token, freq) in token_frequencies.items() if is_valid_token(token, token_frequencies)}))\n",
    "\n",
    "    for doc_id, terms in term_document_matrix.items():\n",
    "        term_document_matrix[doc_id] = {token: freq for token, freq in terms.items() if is_valid_token(token, token_frequencies)}\n",
    "\n",
    "    return token_frequencies, term_document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequencies, term_document_matrix = get_freqs(train)\n",
    "token_frequencies.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../assets/data/')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(data_dir / 'token_frequencies.tsv', 'w', encoding='utf-8') as file:\n",
    "    for token, freq in token_frequencies.items():\n",
    "        if is_valid_token(token, token_frequencies):\n",
    "            file.write(f'{token}\\t{freq}\\n')\n",
    "\n",
    "with open(data_dir / 'term_document_matrix.tsv', 'w', encoding='utf-8') as file:\n",
    "    for doc_id, terms in term_document_matrix.items():\n",
    "        for token, freq in terms.items():\n",
    "            file.write(f'{doc_id}\\t{token}\\t{freq}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../assets/data/')\n",
    "\n",
    "token_frequencies = {}\n",
    "with open(data_dir / 'token_frequencies.tsv', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        token, freq = line.strip().split('\\t')\n",
    "        token_frequencies[token] = int(freq)\n",
    "\n",
    "term_document_matrix = {}\n",
    "with open(data_dir / 'term_document_matrix.tsv', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        doc_id, token, freq = line.strip().split('\\t')\n",
    "        doc_id = int(doc_id)\n",
    "        freq = int(freq)\n",
    "        if doc_id not in term_document_matrix:\n",
    "            term_document_matrix[doc_id] = {}\n",
    "        term_document_matrix[doc_id][token] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_document_vector(token, term_document_matrix):\n",
    "    vector = []\n",
    "    for k, v in term_document_matrix.items():\n",
    "        freq = v.get(token, 0)\n",
    "        vector.append(freq)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_term_document_vector('Reuters', term_document_matrix)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_term_document_vector('cat', term_document_matrix)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    def split_into_sentences(text):\n",
    "        # so the website will not split into two separate sentences by comma:\n",
    "        sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)(?=\\s|[#])')\n",
    "        sentences = sentence_endings.split(text)\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "        return sentences\n",
    "    \n",
    "    def split_into_words(sentences):\n",
    "        # regular expression to match complex URLs, simple URLs, hashtags, Twitter handles, and words\n",
    "        word_pattern = re.compile(r'pic.twitter.com/\\S+|https?://\\S+|www\\.\\S+|\\#\\S+|\\@\\w+|\\b\\w+\\'?\\w*|-?\\w+\\'?\\w*')\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            words = word_pattern.findall(sentence)\n",
    "            tokenized_sentences.append(words)\n",
    "        return tokenized_sentences\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    tokenized = split_into_words(sentences)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentence_tokens, token):\n",
    "    return sentence_tokens.count(token) / len(sentence_tokens)\n",
    "\n",
    "\n",
    "def compute_idf(token, term_document_matrix, total_documents):\n",
    "    doc_count = sum(1 for doc in term_document_matrix if token in term_document_matrix[doc])\n",
    "    return log1p(total_documents / (1 + doc_count))\n",
    "\n",
    "\n",
    "def process_text_and_create_matrices(text, token_frequencies, term_document_matrix):\n",
    "    tokenized_sentences = preprocess_text(text)\n",
    "    total_documents = len(term_document_matrix)\n",
    "    vocabulary = sorted(token_frequencies.keys())\n",
    "\n",
    "    max_sentence_length = max(len(sentence) for sentence in tokenized_sentences)\n",
    "\n",
    "    frequency_matrix = []\n",
    "    tfidf_matrix = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence_freq_vector = [0] * max_sentence_length\n",
    "        sentence_tfidf_vector = [0] * max_sentence_length\n",
    "\n",
    "        for i, token in enumerate(sentence):\n",
    "            if token in vocabulary:\n",
    "                tf = compute_tf(sentence, token)\n",
    "                idf = compute_idf(token, term_document_matrix, total_documents)\n",
    "\n",
    "                sentence_freq_vector[i] = tf\n",
    "                sentence_tfidf_vector[i] = tf * idf\n",
    "\n",
    "        frequency_matrix.append(sentence_freq_vector)\n",
    "        tfidf_matrix.append(sentence_tfidf_vector)\n",
    "\n",
    "    frequency_matrix = np.array(frequency_matrix)\n",
    "    tfidf_matrix = np.array(tfidf_matrix)\n",
    "\n",
    "    document_vector_freq = np.mean(frequency_matrix, axis=0)\n",
    "    document_vector_tfidf = np.mean(tfidf_matrix, axis=0)\n",
    "\n",
    "    return document_vector_freq, document_vector_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Boos and chants of  Lock her up!  were heard in the crowd assembled at the West Front of the U.S. Capitol Friday morning when defeated Democratic Party presidential nominee Hillary Clinton was introduced at the inaugural ceremony for President-elect Donald Trump.#InaugurationDay Lock her up pic.twitter.com/APVtyyYote  Bill Simms (@Mittens1245) January 20, 2017The crowd on the mall booed when the jumbotron showed a close-up shot of Hillary Clinton at #Inauguration https://t.co/1dvY5lxdKo  gpbnews (@gpbnews) January 20, 2017Some in crowd chanting LOCK HER UP as Hillary Clinton arrives  Jamie Dupree (@jamiedupree) January 20, 2017Via: Gateway Pundit '\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector_freq, document_vector_tfidf = process_text_and_create_matrices(text, token_frequencies, term_document_matrix)\n",
    "document_vector_freq.shape, document_vector_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [[token for token in ds['Token'].to_list() if token in token_frequencies.keys() and is_valid_token(token, token_frequencies)] for ds in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=train_texts, vector_size=30, window=5, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../models/').mkdir(parents=True, exist_ok=True)\n",
    "model_path = '../models/word2vec.model'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word:', token_frequencies['Monday'])\n",
    "print('Close:', token_frequencies['Tuesday'], token_frequencies['Wednesday'], token_frequencies['Thursday'])\n",
    "print('Same area', token_frequencies['weekend'], token_frequencies['day'], token_frequencies['week'])\n",
    "print('Other semantic', token_frequencies['funds'], token_frequencies['town'], token_frequencies['territory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word:', token_frequencies['north'])\n",
    "print('Close:', token_frequencies['south'], token_frequencies['west'], token_frequencies['east'])\n",
    "print('Same area', token_frequencies['world'], token_frequencies['side'], token_frequencies['direction'])\n",
    "print('Other semantic', token_frequencies['party'], token_frequencies['senator'], token_frequencies['husband'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word:', token_frequencies['Spain'])\n",
    "print('Close:', token_frequencies['Madrid'], token_frequencies['Catalonia'], token_frequencies['Europe'])\n",
    "print('Same area', token_frequencies['Brexit'], token_frequencies['kingdom'], token_frequencies['EU'])\n",
    "print('Other semantic', token_frequencies['Trump'], token_frequencies['Twitter'], token_frequencies['Korea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "words_to_analyze = ['Monday', 'north', 'Spain']\n",
    "similar_words = {\n",
    "    'Monday': ['Tuesday', 'Wednesday', 'Thursday'], \n",
    "    'north': ['south', 'west', 'east'],\n",
    "    'Spain': ['Madrid', 'Catalonia', 'Europe']\n",
    "}\n",
    "\n",
    "related_words = {\n",
    "    'Monday': ['weekend', 'day', 'week'], \n",
    "    'north': ['world', 'side', 'direction'],\n",
    "    'Spain': ['Brexit', 'kingdom', 'EU']\n",
    "}\n",
    "\n",
    "unrelated_words = {\n",
    "    'Monday': ['funds', 'town', 'territory'], \n",
    "    'north': ['party', 'senator', 'husband'],\n",
    "    'Spain': ['Trump', 'Twitter', 'Korea']\n",
    "}\n",
    "\n",
    "for word in words_to_analyze:\n",
    "    word_vec = model.wv[word]\n",
    "    print(f'Cosine distances for \"{word}\":')\n",
    "    for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "        distances = {target_word: cosine_similarity(word_vec, model.wv[target_word]) for target_word in words[word]}\n",
    "        print(f'\\t{group}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_to_analyze:\n",
    "    word_vec = get_term_document_vector(word, term_document_matrix)\n",
    "    print(f'Cosine distances for \"{word}\":')\n",
    "    for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "        distances = {target_word: cosine_similarity(word_vec, get_term_document_vector(target_word, term_document_matrix)) for target_word in words[word]}\n",
    "        print(f'\\t{group}: {distances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_df = np.zeros((len(token_frequencies), len(term_document_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, term in enumerate(token_frequencies.keys()):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    term_document_df[i, :] = np.array(get_term_document_vector(term, term_document_matrix), dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_document_df_ = term_document_df[:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "n_components = 30\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_tfidf_vectors = pca.fit_transform(term_document_df)\n",
    "reduced_tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../assets/reduced_tfidf_vectors.npy', reduced_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_vectors = pd.DataFrame.from_records(reduced_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_vectors.index = list(token_frequencies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_tfidf_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_to_analyze:\n",
    "    try:\n",
    "        word_vec = reduced_tfidf_vectors.loc[word]\n",
    "        print(f'Cosine distances for \"{word}\":')\n",
    "        for group, words in [('Similar', similar_words), ('Related', related_words), ('Unrelated', unrelated_words)]:\n",
    "            distances = {target_word: cosine_similarity(word_vec, reduced_tfidf_vectors.loc[target_word]) for target_word in words[word]}\n",
    "            print(f'\\t{group}: {distances}')\n",
    "    except:\n",
    "        print('no words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_with_w2v(text, model):\n",
    "    tokenized_sentences = preprocess_text(text)\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in model.wv.key_to_index:\n",
    "                word_vector = model.wv[word]\n",
    "                word_vectors.append(word_vector)\n",
    "                \n",
    "        if word_vectors:\n",
    "                sentence_vector = np.mean(word_vectors, axis=0)\n",
    "                sentence_vectors.append(sentence_vector)\n",
    "\n",
    "    if sentence_vectors:\n",
    "        document_vector = np.mean(sentence_vectors, axis=0)\n",
    "        return document_vector\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train_texts[0])\n",
    "print(text)\n",
    "print(vectorize_with_w2v(text, model).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [[token for token in ds['Token'].to_list() if token in token_frequencies.keys() and is_valid_token(token, token_frequencies)] for ds in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = [vectorize_with_w2v(' '.join(text), model) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/annotated-corpus/test-embeddings.tsv', 'w') as file:\n",
    "    for doc_id, vector in zip(test_ids, test_vectors):\n",
    "        vector_str = '\\t'.join(map(str, vector))\n",
    "        file.write(f'{doc_id}\\t{vector_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
