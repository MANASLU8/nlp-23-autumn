{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory work #2 (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import TrigramCollocationFinder, TrigramAssocMeasures\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.errors import EmptyDataError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(root_dir, n=None):\n",
    "    file_paths = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.tsv'):\n",
    "                file_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    all_data = []\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if n is not None and i >= n:\n",
    "            break\n",
    "        try:\n",
    "            rows = []\n",
    "            sentence_index = 0\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        token, stem, lemma = line.split('\\t')\n",
    "                        rows.append({'Token': token, 'Stem': stem, 'Lemma': lemma, 'Sentence_Index': sentence_index})\n",
    "                    else:\n",
    "                        sentence_index += 1\n",
    "\n",
    "            file_data = pd.DataFrame(rows)\n",
    "            if len(file_data) >= 1:\n",
    "                all_data.append(file_data)\n",
    "        except EmptyDataError as e:\n",
    "            print(i, file_path, e)\n",
    "    ids = [os.path.splitext(os.path.basename(path))[0] for path in file_paths]\n",
    "    return ids, all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, train = read_files('../assets/annotated-corpus/train', \n",
    "                #    1000\n",
    "                   )\n",
    "# val_ids, val = read_files('../assets/annotated-corpus/val', \n",
    "#                 #  100\n",
    "#                  )\n",
    "test_ids, test = read_files('../assets/annotated-corpus/test', \n",
    "                #   100\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[1].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_freq(df_list, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    \n",
    "    n_grams = []\n",
    "    for df in df_list:\n",
    "        try:\n",
    "            stems = df['Stem'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)).str.lower()\n",
    "            stems = [stem for stem in stems if stem not in stop_words and stem.strip() != '']\n",
    "            trigrams = list(ngrams(stems, 3))\n",
    "            n_grams.extend(trigrams)\n",
    "        except:\n",
    "            print('error with df:')\n",
    "            print(df)\n",
    "\n",
    "    n_gram_freq = Counter(n_grams)\n",
    "    return n_gram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq = get_n_gram_freq(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MI(n_grams, total_words, word_freq):\n",
    "    mi_scores = {}\n",
    "    for n_gram in n_grams:\n",
    "        p_n_gram = n_grams[n_gram]\n",
    "        p_w1 = word_freq[n_gram[0]]\n",
    "        p_w2 = word_freq[n_gram[1]] \n",
    "\n",
    "        mi_score = math.log2(p_n_gram * pow(total_words, 2) / (p_w1 * p_w2))\n",
    "        mi_scores[n_gram] = mi_score\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def get_mi_scores(freq):\n",
    "    total_words = sum(freq.values())\n",
    "    word_freq = FreqDist(word for trigram in freq for word in trigram)\n",
    "    mi_scores = calculate_MI(freq, total_words, word_freq)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def get_mi_scores_nltk(df_list, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    full_text = ''\n",
    "    \n",
    "    for df in df_list:\n",
    "        try:\n",
    "            words = df['Stem'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)).str.lower()\n",
    "            words = [word for word in words if word not in stop_words and word.strip() != '']\n",
    "            full_text += ' '.join(words)\n",
    "        except:\n",
    "            print('error with df:')\n",
    "            print(df)\n",
    "        \n",
    "    tokens = nltk.word_tokenize(full_text, language, True)\n",
    "    text = nltk.Text(tokens)\n",
    "        \n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    finder = TrigramCollocationFinder.from_words(text)\n",
    "    nltk_mi_scores = finder.score_ngrams(trigram_measures.pmi)\n",
    "    return nltk_mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mi_scores = get_mi_scores(train_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mi_scores_nltk = get_mi_scores_nltk(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "sorted_mi_scores = sorted(train_mi_scores.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "print(f'Top {n} trigrams MI:')\n",
    "for trigram, score in sorted_mi_scores:\n",
    "    print(f'{trigram}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "sorted_mi_scores = sorted(train_mi_scores.items(), key=lambda x: x[1], reverse=False)\n",
    "print(f'Last top {n} trigrams MI:')\n",
    "for trigram, score in sorted_mi_scores[:n]:\n",
    "    print(f'{trigram}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top {n} trigrams MI with nltk:')\n",
    "for trigram, score in train_mi_scores_nltk[:n]:\n",
    "    print(f'{trigram}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_mi_dict = dict(train_mi_scores_nltk)\n",
    "\n",
    "matched_mi_scores = []\n",
    "\n",
    "for trigram, mi_score in sorted_mi_scores:\n",
    "    if trigram in nltk_mi_dict:\n",
    "        matched_mi_scores.append((trigram, mi_score, nltk_mi_dict[trigram]))\n",
    "\n",
    "# for trigram, own_mi, nltk_mi in matched_mi_scores:\n",
    "#     print(f'Trigram: {trigram}, My MI: {own_mi}, NLTK MI: {nltk_mi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(matched_mi_scores, columns=['Trigram', 'MI', 'NLTK_MI']).iloc[:1000, :]\n",
    "x_indexes = range(len(df))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(x_indexes, df['MI'], label='MI calculation', marker='o')\n",
    "plt.plot(x_indexes, df['NLTK_MI'], label='NLTK MI calculation', marker='x')\n",
    "\n",
    "plt.xlabel('Trigram Index')\n",
    "plt.ylabel('MI Score')\n",
    "plt.title('Comparison of MI scores between own calculation and NLTK')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(matched_mi_scores, columns=['Trigram', 'MI', 'NLTK_MI'])\n",
    "x_indexes = range(len(df))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(x_indexes, df['NLTK_MI'], label='NLTK MI calculation', marker='x', alpha=0.1)\n",
    "plt.plot(x_indexes, df['MI'], label='MI calculation', marker='o', alpha=0.9)\n",
    "\n",
    "plt.xlabel('Trigram Index')\n",
    "plt.ylabel('MI Score')\n",
    "plt.title('Comparison of MI scores between own calculation and NLTK')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
