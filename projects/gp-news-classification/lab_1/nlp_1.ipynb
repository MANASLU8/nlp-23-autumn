{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d57caa4259352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.join('data.csv'), names=['label', 'Title', 'Description'])\n",
    "df['text'] = (df['Title'] + '. ' + df['Description'])\n",
    "df.drop(columns=['Title', 'Description'], axis=1, inplace=True)\n",
    "print(df.head())\n",
    "print(df['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c6f57643b7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b78e23c14172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_to_sent(text):\n",
    "    sentences = re.split(\n",
    "        r\"(((?<!\\w\\.\\w.)(?<!\\s\\w\\.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s(?=[A-Z]))|((?<![\\,\\-\\:])\\n(?=[A-Z]|\\\" )))\", text)[\n",
    "                ::4]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded99f420ae9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_words(sentence):\n",
    "    words = re.findall(r\"\\w+@\\w+\\.\\w+|\\+\\d{1,3}-\\d{3}-\\d{3}-\\d{2}-\\d{2}|\\w+\", sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4a0464b92971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def process_file():\n",
    "    wnl = WordNetLemmatizer()\n",
    "    sst = SnowballStemmer(\"english\")\n",
    "    counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        sentences = split_to_sent(row['text'])\n",
    "        words_dic = []\n",
    "        counter += 1\n",
    "        for s in sentences:\n",
    "            words_dic += split_to_words(s)\n",
    "            words_dic.append(\"\\n\")\n",
    "        lemmatized = []\n",
    "        stemmed = []\n",
    "        original = []\n",
    "        for w in words_dic:\n",
    "            w_processed = re.sub(r\"[.!?,]$\", \"\", w).lower() # убрать удаление точек и etc. отделить от токена и сохранить отдлеьно (1 балл доделать)\n",
    "            lemmatized.append(wnl.lemmatize(w_processed))\n",
    "            stemmed.append(sst.stem(w_processed))\n",
    "            original.append(w_processed)\n",
    "        save_to_file(original, lemmatized, stemmed, os.path.join(\".\", \"assets\", \"annotated-corpus\", \"train\", str(row['label']), str(counter) + \".tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae86957a0812dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(original, lemmatized, stemmed, id):\n",
    "    with open(id, \"w\") as f:\n",
    "        for i in range(len(original)):\n",
    "            if original[i] == \"\\n\":\n",
    "                 print(\"\", file=f)\n",
    "            else:\n",
    "                print(original[i], stemmed[i], lemmatized[i], sep=\"\\t\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8c43fc3a82676",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
