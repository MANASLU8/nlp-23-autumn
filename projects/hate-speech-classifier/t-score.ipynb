{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import SnowballStemmer\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "# from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_RAW_PATH = os.path.realpath(\"./assets/data/raw-data/labeled.csv\")\n",
    "LABEL_X = \"comment\"\n",
    "LABEL_Y = \"toxic\"\n",
    "DATASET_LANG = \"russian\"\n",
    "\n",
    "LOG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str, extract_method: typing.Callable = pd.read_csv, **pandas_kwargs: dict[str, any]) -> typing.Collection:\n",
    "    df = extract_method(data_path, **pandas_kwargs)\n",
    "    if LOG:\n",
    "        print(\"Data loaded!   Shape is:\", df.shape)\n",
    "        print(df.head(), \"\\n\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents = load_data(DATASET_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_corpus(documents: pd.DataFrame, lemmatize: typing.Callable[[str], str], stopwords_list: typing.Iterable[str]):\n",
    "    word_pattern = re.compile(r\"[А-Яа-яЁёA-Za-z]+\")\n",
    "    ret = []\n",
    "    for doc in documents:\n",
    "        doc = doc.lower()\n",
    "        doc_lemmas = [lemmatize(token.group()) for token in word_pattern.finditer(doc) if token.group() not in stopwords_list]\n",
    "        ret.append(doc_lemmas)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_analyzer = MorphAnalyzer()\n",
    "lemmatize = lambda token: morph_analyzer.normal_forms(token)[0]\n",
    "lemmatized_docs = lemmatize_corpus(text_documents[LABEL_X], lemmatize=lemmatize, stopwords_list = stopwords.words(DATASET_LANG))\n",
    "lemmatized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_score_fn(trig: tuple[str], trig_freq: int, token_freqs: dict[str, int], n: int) -> float:\n",
    "    unig_prod = token_freqs[trig[0]]\n",
    "    for i in range(1, len(trig)):\n",
    "        unig_prod *= token_freqs[trig[i]]\n",
    "\n",
    "    return (trig_freq - (unig_prod /( n ** 2))) / (trig_freq ** .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_freqs(docs: list[list[str]]) -> dict[str, int]:\n",
    "    token_freqs = Counter()\n",
    "    for d in docs:\n",
    "        token_freqs.update(d)\n",
    "    return token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trig_freqs(docs: list[list[str]]) -> dict[tuple[str], int]:\n",
    "    trig_freqs = Counter()\n",
    "    for d in docs:\n",
    "        for trig_idx in range(len(d) - 2):\n",
    "            trig_freqs[tuple(d[trig_idx : trig_idx + 3])] += 1\n",
    "    \n",
    "    return trig_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams_tsocres(docs: list[list[str]]) -> dict[tuple[str], float]:\n",
    "    scores = {}\n",
    "    trig_freqs = get_trig_freqs(docs)\n",
    "    token_freqs = get_token_freqs(docs)\n",
    "    n = 0\n",
    "    for d in docs:\n",
    "        n += len(d)\n",
    "    \n",
    "    for trig, trig_freq in trig_freqs.items():\n",
    "        scores[trig] = t_score_fn(trig, trig_freq, token_freqs, n)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(scores: dict[tuple[str], float], k=10) -> list:\n",
    "    return list(sorted(scores.items(), key = lambda x: x[1], reverse=True)[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = make_trigrams_tsocres(lemmatized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k(scores, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder_thr = TrigramCollocationFinder.from_documents(lemmatized_docs)\n",
    "finder_thr.nbest(trigram_measures.student_t, 30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
