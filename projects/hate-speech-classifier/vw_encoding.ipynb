{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e35c2d93e3339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from math import log10\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from nltk import SnowballStemmer\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_INPUT_DIR = os.path.realpath(\"./assets/data/annotaed-tsv/\")\n",
    "DATASET_OUTPUT_DIR = os.path.realpath(\"./assets/wordcount/train\")\n",
    "DATASET_EMBEDDINGS_TEST_DIR = os.path.realpath(\"./assets/embeddings/test/\")\n",
    "DATASET_EMBEDDINGS_TRAIN_DIR = os.path.realpath(\"./assets/embeddings/train/\")\n",
    "MODEL_ARTIFACTS_DIR = os.path.realpath(\"./assets/artifacts/\")\n",
    "\n",
    "TOKEN_FREQS_JSON_PATH = os.path.join(DATASET_OUTPUT_DIR, \"token_frequencies.json\")\n",
    "TERM_DOC_PATH = os.path.join(DATASET_OUTPUT_DIR, \"term_document.npz\")\n",
    "TOKEN_ORDER_PATH = os.path.join(DATASET_OUTPUT_DIR, \"token_order.json\")\n",
    "\n",
    "DOCS_TRAIN_TEST_NAMES_FILE = os.path.join(DATASET_INPUT_DIR, \"train_test_lists.json\")\n",
    "LOAD_TRAIN_TEST_FROM_JSON = True\n",
    "\n",
    "DATASET_LANG = \"russian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d473ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_file_paths = glob.glob(os.path.join(DATASET_INPUT_DIR, \"*/*.tsv\"))\n",
    "docs_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_TRAIN_TEST_FROM_JSON:\n",
    "    with open(DOCS_TRAIN_TEST_NAMES_FILE) as f:\n",
    "        dataset_train_test_config = json.load(f)\n",
    "\n",
    "    docs_file_paths_test = dataset_train_test_config[\"test\"]\n",
    "    docs_file_paths_train = dataset_train_test_config[\"train\"]\n",
    "else:\n",
    "    docs_file_paths_test = set(random.sample(docs_file_paths, k=int(len(docs_file_paths) * 0.3)))\n",
    "    docs_file_paths_train = list(set(docs_file_paths) - docs_file_paths_test)\n",
    "    docs_file_paths_test = list(docs_file_paths_test)\n",
    "    \n",
    "    with open(DOCS_TRAIN_TEST_NAMES_FILE, \"w\") as f:\n",
    "        json.dump({\"train\": docs_file_paths_train, \"test\": docs_file_paths_test}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_number_regex = r\"(\\+\\d{1,3})?\\s?\\(?\\d{1,4}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\"\n",
    "email_regex = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\"\n",
    "word_regex = r\"([А-Яа-яЁёA-Za-z0-9-]+)\"\n",
    "token_pattern = re.compile(\"|\".join([\n",
    "    email_regex,\n",
    "    phone_number_regex,\n",
    "    word_regex,\n",
    "]))\n",
    "\n",
    "def get_stems_processed(path: str) -> list[list[str]]:\n",
    "    sentences = []\n",
    "    with open(path) as f:\n",
    "        lines = \"\".join(f.readlines())\n",
    "        sentences_raw = lines.split(\"\\n\\t\\t\\n\")\n",
    "        for sentence in sentences_raw:\n",
    "            stems = []\n",
    "            words = sentence.split(\"\\n\")\n",
    "            if len(words) == 0 or words[0] == \"\":\n",
    "                continue\n",
    "            stems_raw = list(map(lambda x: x.split(\"\\t\")[1], words))\n",
    "            lemmas = list(map(lambda x: x.split(\"\\t\")[2], words))\n",
    "            for i in range(len(stems_raw)):\n",
    "                if lemmas[i] not in stopwords.words(DATASET_LANG) and token_pattern.match(lemmas[i]) is not None:\n",
    "                    stems.append(stems_raw[i])\n",
    "            sentences.append(stems)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5021cd9226debb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_docs(sentences) -> list[dict[str, int]]:\n",
    "    sentences_count = []\n",
    "    for sentence in sentences:\n",
    "        token_by_sent_count = Counter()\n",
    "        for word in sentence:\n",
    "            token_by_sent_count[word] += 1\n",
    "        sentences_count.append(token_by_sent_count)\n",
    "    return sentences_count\n",
    "\n",
    "def count_words_in_docs(sentences: list[list[str]], count_by_sentences=False) -> dict[str, int] | list[dict[str, int]]:\n",
    "    if count_by_sentences:\n",
    "        return count_words_in_docs(sentences)\n",
    "    \n",
    "    token_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            token_count[word] += 1\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110b36a52c8306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_stems(docs_file_paths: list[str]) -> tuple[dict[str, int], list[dict[str, int]], list[list[list[str]]]]:\n",
    "    docs = []\n",
    "    token_freqs = Counter()\n",
    "    token_freq_by_doc = []\n",
    "    \n",
    "    for filename in tqdm(docs_file_paths):\n",
    "        sents = get_stems_processed(filename)\n",
    "        docs.append(sents)\n",
    "        counts = count_words_in_docs(sents)\n",
    "\n",
    "        token_freq_by_doc.append(counts)\n",
    "        token_freqs.update(counts)\n",
    "\n",
    "    return token_freqs, token_freq_by_doc, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7185d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_freqs(freqs: dict[str, int], threshhold: int) -> dict[str, int]:\n",
    "    return {token: freq for token, freq in freqs.items() if freq >= threshhold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8449c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_freqs(freqs: dict[str, int], path: str) -> None:\n",
    "    dir = \"\".join(path.split(\"/\")[:-1])\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(freqs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bb900731d4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs, token_freqs_by_doc, all_docs = count_all_stems(docs_file_paths_train)\n",
    "token_freqs = filter_freqs(token_freqs, 5)\n",
    "\n",
    "token_freqs_by_doc = [{token: freq for token, freq in doc_freqs.items() if token_freqs.get(token, False)} for doc_freqs in token_freqs_by_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43163f1a726311",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_freqs(token_freqs, TOKEN_FREQS_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_term_doc_matrix(token_freqs: dict[str, int], token_freqs_by_doc: list[dict[str, int]]) -> tuple[np.matrix, dict[str, int]]:\n",
    "    raw_mat = np.zeros((len(token_freqs_by_doc), len(token_freqs)), dtype=np.uint16)\n",
    "    term_dict = {token: id for id, token in enumerate(token_freqs.keys())}\n",
    "\n",
    "    for doc_i, doc_token_freqs in enumerate(token_freqs_by_doc):\n",
    "        for token, freq in doc_token_freqs.items():\n",
    "            raw_mat[doc_i][term_dict[token]] = freq\n",
    "    \n",
    "    return raw_mat, term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_mat(matrix: np.ndarray, path: str) -> None:\n",
    "    sp_mat = sparse.bsr_array(matrix, dtype=matrix.dtype)\n",
    "    sparse.save_npz(path, sp_mat)\n",
    "\n",
    "def load_sparse_mat(path: str, ) -> np.ndarray:\n",
    "    sp_mat: sparse.sparray = sparse.load_npz(path)\n",
    "    return sp_mat.toarray()\n",
    "\n",
    "def save_token_ordering_dict(order_dict: dict[str, int], path: str) -> None:\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(order_dict, f)\n",
    "\n",
    "def load_ordering_dict(path: str) -> dict[str, int]:\n",
    "    with open(path) as f:\n",
    "        order_dict = json.load(f)\n",
    "    return order_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5214e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_martix, term_doc_token_id_dict = make_term_doc_matrix(token_freqs, token_freqs_by_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bac591",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sparse_mat(term_doc_martix, TOKEN_ORDER_PATH)\n",
    "save_token_ordering_dict(term_doc_token_id_dict, TOKEN_ORDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(data=term_doc_martix, columns=sorted(term_doc_token_id_dict, key=term_doc_token_id_dict.get))\n",
    "matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9dcb672e6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(words: dict[str, int], matrix: pd.DataFrame) -> list[float]:\n",
    "    total_words = sum(words.values())\n",
    "    total_documents = len(matrix.index)\n",
    "    result = []\n",
    "    for w in matrix.columns:\n",
    "        if w not in words:\n",
    "            result.append(0.0)\n",
    "            continue\n",
    "        t_f = words[w] / total_words\n",
    "        d_f = sum(matrix[w] > 0)\n",
    "        tfidf = t_f * (log10(total_documents + 1) - log10(d_f + 1))\n",
    "        result.append(tfidf)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(text: str) -> list[str]:\n",
    "    sentences = re.split(\n",
    "        r\"(((?<!\\w\\.\\w.)(?<!\\s\\w\\.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s(?=[A-Z]))|((?<![\\,\\-\\:])\\n(?=[A-Z]|\\\" )))\", text)[\n",
    "                ::4]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_to_words(sentence: str) -> list[str]:\n",
    "    words = re.findall(r\"\\w+@\\w+\\.\\w+|\\+\\d{1,3}-\\d{3}-\\d{3}-\\d{2}-\\d{2}|\\w+\", sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa70e8abb5a96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, by_sentences=False) -> list[str] | list[list[str]]:\n",
    "    stemmer = SnowballStemmer(DATASET_LANG)\n",
    "    lemmatizer = MorphAnalyzer()\n",
    "    sentences = split_to_sentences(text)\n",
    "    result = []\n",
    "    for s in sentences:\n",
    "        sentence = []\n",
    "        for w in split_to_words(s):\n",
    "            w_processed = re.sub(r\"[.!?,]$\", \"\", w).lower()\n",
    "            lemma = lemmatizer.normal_forms(w_processed)[0]\n",
    "            if lemma not in stopwords.words(DATASET_LANG):\n",
    "                sentence.append(stemmer.stem(w_processed))\n",
    "        if by_sentences:\n",
    "            result.append(sentence)\n",
    "        else:\n",
    "            result += sentence\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7fdae298659b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tf_idf(text: str, matrix: np.matrix):\n",
    "    preprocessed = preprocess_text(text)\n",
    "    text_dict = count_words_in_docs([preprocessed])\n",
    "    return tf_idf(text_dict, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd61b3172b8d9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_1 = \"В обеих странах есть партия войны . И в обеих странах эта война сейчас разжигается и поддерживается Смотрите, парни. И вот после этого руснявого пиздежа пидараны требуют к ним хорошего отношения? Мань, может это Украина хуярит по в на роисе градами ? Украина засылает в на роисю террористов бандитов ихтамнетов? Харк тебе в ебло, спидозная тварь.\"\n",
    "example_doc_2 = \"Возьмём как пример Россию, западноевропейские страны и США. Идёт метисация, сознательная политика замещения белого населения на пришлое черно-коричневое. Идёт создание новой расы метисов, исламизация и почернение. В крупных городах половина населения - выходцы из ебеней Мексики, Африки, Ближнего Востока, а в случае с Россией - Кавказа и Средней Азии. Этнические ниггеро-арабские гетто верят на хую законы как хотят, чудовищная по масштабам этническая преступность. Говорить о миграции и тем более затрагивать тему замещения коренного населения властями нельзя, иначе бутылка. Свобода слова тут не для вас, молодой человек. При этом говорить о том, что белые должны вымереть, и это нормально - можно. Белые официально вымирают ведётся пропаганда так или иначе направленная на снижение рождаемости белого населения. Феминизм, ЛГБТ, чайлдфри. Каждая женщина в Швеции - леволиберальная феминистка, это страна победившего феминизма. Что сегодня там происходит - страшно делается. Пропагандируются смешанные браки, межрасовые браки, пропагандируется превосходство детей-метисов. Идёт демонизация белых и пропаганда превосходства чёрных и смуглых мужчин, форс отношений белая женщина смуглый чёрный мужчина-мигрант. Как результат - всё больше чернильниц, всё больше смешанных браков, всё больше небелых метисов. Белые женщины просто не хотят контактировать с мужчинами своей нации и расы, наделяя их самыми плохими качествами и обожествляя черных. При этом большинство белых не считает завоз чурок чем-то плохим, наоборот, относятся к ним толерантно. Проводится политика насаждения толерантности, мультикультурализма, политкорректности и космополитизма. Набирающее популярность даже в России SJW - это вообще отдельная тема для обсуждения. Всё вышеперечисленное относится к сильнейшим когда-то странам, бывшим империям, нагибающим слабых. Сегодня происходит так, что бывшие империи в прямом смысле деградируют, вырождаются и вымирают, а место сильнейших когда-то, господствующих народов, занимают те, кого когда-то колонизировали. Во Франции к 2080 уже будут доминировать негры и арабы, в России - кавказцы и выходцы из средней Азии, в Великобритании - индийцы, негры, арабы, пакистанцы, etc. А в маленьких, нейтральных странах, вроде Словении или Беларуси, Литвы или Чехии, Румынии или Эстонии - всё пучком. Им вымирание не грозит, они остаются и будут оставаться белыми. Более того, у них ведётся политика, направленная на сохранение традиционных ценностей и культуры коренного населения. Они сказали беженцам нет . В Польшу, например, русскому или украинцу гораздо легче переехать и остаться, чем арабу или африканцу. В Германии ситуация противоположная, белых там не ждут. Польша, Чехия, Словакия, Венгрия, Словения, Хорватия, Сербия, БиГ, Черногория, Македония, Греция, Болгария, Румыния, Молдова, Украина, Беларусь, Литва, Латвия, Эстония - вот Европа будущего. Скандинавия, Южная, Западная Европа, а также Россия - лишатся коренного населения и своей культуры.\"\n",
    "\n",
    "print(vectorize_tf_idf(example_doc_1, matrix))\n",
    "print(vectorize_tf_idf(example_doc_2, matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(word: str) -> str:\n",
    "    stemmer = SnowballStemmer(DATASET_LANG)\n",
    "    token = stemmer.stem(word)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a868997f2a78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec(sentences=[sentence for document in all_docs for sentence in document], epochs=50)\n",
    "os.makedirs(MODEL_ARTIFACTS_DIR, exist_ok=True)\n",
    "model_w2v.save(os.path.join(MODEL_ARTIFACTS_DIR, \"w2v_weights__last\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce14f862dcbacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.most_similar(tokenize_word(\"политика\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644db1b68e6d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44368cd7f4aec1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(\n",
    "    model_w2v.wv[tokenize_word(\"Путин\")],\n",
    "    model_w2v.wv[tokenize_word(\"Европа\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9b4f634e29e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_words(terms, vectors_source):\n",
    "    pca = PCA(n_components=2)\n",
    "    vectors_2d = pd.DataFrame(pca.fit_transform([vectors_source[term] for term in terms]))\n",
    "    vectors_2d.index = terms\n",
    "    vectors_2d.columns = [\"x\", \"y\"]\n",
    "    p = sns.scatterplot(data=vectors_2d, x=\"x\", y=\"y\")\n",
    "\n",
    "    for i in vectors_2d.index:\n",
    "        item = vectors_2d.loc[i]\n",
    "        p.text(item.x, item.y, i)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad3a591c7a4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_check = [\n",
    "    \"политика\",\n",
    "    \"США\",\n",
    "    \"русский\",\n",
    "    \"высказать\",\n",
    "    \"оскорбления\",\n",
    "    \"создавать\",\n",
    "    \"Европу\",\n",
    "    \"техника\",\n",
    "    \"религии\",\n",
    "    \"город\",\n",
    "    \"Путин\",\n",
    "    \"Трамп\",\n",
    "]\n",
    "\n",
    "terms_to_check_tokenized = [tokenize_word(tok) for tok in terms_to_check]\n",
    "draw_words(terms_to_check_tokenized, model_w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ffa81a7262ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_compare(vectors: list[list[str]]) -> np.ndarray:\n",
    "    pca = PCA(n_components=len(model_w2v.wv[0]))\n",
    "    transformed = pca.fit_transform(vectors)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tf_idf_tokens(matrix: list[str]) -> list[list[float]]:\n",
    "    result = [None for _ in range(len(matrix.columns))]\n",
    "    for i, token in tqdm(enumerate(matrix.columns)):\n",
    "        result[i] = vectorize_tf_idf(token, matrix)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2500ff8a80d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_vectorized = [vectorize_tf_idf(token, matrix) for token in matrix.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11173fe88a13984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cmp = transform_to_compare(terms_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879f31ce86d9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_compare = pd.DataFrame(to_cmp)\n",
    "terms_to_compare.index = matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391dc72628e98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(token_1, token_2):\n",
    "    print(f\"Косинусные расстояния между токенами {token_1} и {token_2} \\nW2V:{cosine_similarity(model_w2v.wv[token_1], model_w2v.wv[token_2])}\\nTf-Idf:{cosine_similarity(terms_to_compare.loc[token_1], terms_to_compare.loc[token_2])}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39e2a38b956be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_methods(\n",
    "    tokenize_word(\"Европа\"),\n",
    "    tokenize_word(\"США\"),\n",
    ")\n",
    "compare_methods(\n",
    "    tokenize_word(\"политика\"),\n",
    "    tokenize_word(\"политолог\"),\n",
    ")\n",
    "compare_methods(\n",
    "    tokenize_word(\"обезьяна\"),\n",
    "    tokenize_word(\"человек\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59991ba89e138f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = {}\n",
    "for i in range(len(matrix.columns)):\n",
    "    tfidf_data[matrix.columns[i]] = terms_vectorized[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b13c3080846ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_words(terms_to_check_tokenized, tfidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80826ce08ee5a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_w2v(sentences: list[list[str]], model_w2v):\n",
    "    result_vec = np.zeros(model_w2v.vector_size)\n",
    "    for s in sentences:\n",
    "        sentence_vec = np.zeros(model_w2v.vector_size)\n",
    "        for token in s:\n",
    "            if model_w2v.wv.has_index_for(token):\n",
    "                sentence_vec += model_w2v.wv[token]\n",
    "        sentence_vec = sentence_vec / len(s) if len(s) > 0 else np.zeros(model_w2v.vector_size)\n",
    "        result_vec += sentence_vec\n",
    "    result_vec = result_vec / len(sentences) if len(sentences) > 0 else np.zeros(model_w2v.vector_size)\n",
    "    return result_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9375749ba3399b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc_1 = \"В обеих странах есть партия войны . И в обеих странах эта война сейчас разжигается и поддерживается Смотрите, парни. И вот после этого руснявого пиздежа пидараны требуют к ним хорошего отношения? Мань, может это Украина хуярит по в на роисе градами ? Украина засылает в на роисю террористов бандитов ихтамнетов? Харк тебе в ебло, спидозная тварь.\"\n",
    "example_doc_2 = \"Возьмём как пример Россию, западноевропейские страны и США. Идёт метисация, сознательная политика замещения белого населения на пришлое черно-коричневое. Идёт создание новой расы метисов, исламизация и почернение. В крупных городах половина населения - выходцы из ебеней Мексики, Африки, Ближнего Востока, а в случае с Россией - Кавказа и Средней Азии. Этнические ниггеро-арабские гетто верят на хую законы как хотят, чудовищная по масштабам этническая преступность. Говорить о миграции и тем более затрагивать тему замещения коренного населения властями нельзя, иначе бутылка. Свобода слова тут не для вас, молодой человек. При этом говорить о том, что белые должны вымереть, и это нормально - можно. Белые официально вымирают ведётся пропаганда так или иначе направленная на снижение рождаемости белого населения. Феминизм, ЛГБТ, чайлдфри. Каждая женщина в Швеции - леволиберальная феминистка, это страна победившего феминизма. Что сегодня там происходит - страшно делается. Пропагандируются смешанные браки, межрасовые браки, пропагандируется превосходство детей-метисов. Идёт демонизация белых и пропаганда превосходства чёрных и смуглых мужчин, форс отношений белая женщина смуглый чёрный мужчина-мигрант. Как результат - всё больше чернильниц, всё больше смешанных браков, всё больше небелых метисов. Белые женщины просто не хотят контактировать с мужчинами своей нации и расы, наделяя их самыми плохими качествами и обожествляя черных. При этом большинство белых не считает завоз чурок чем-то плохим, наоборот, относятся к ним толерантно. Проводится политика насаждения толерантности, мультикультурализма, политкорректности и космополитизма. Набирающее популярность даже в России SJW - это вообще отдельная тема для обсуждения. Всё вышеперечисленное относится к сильнейшим когда-то странам, бывшим империям, нагибающим слабых. Сегодня происходит так, что бывшие империи в прямом смысле деградируют, вырождаются и вымирают, а место сильнейших когда-то, господствующих народов, занимают те, кого когда-то колонизировали. Во Франции к 2080 уже будут доминировать негры и арабы, в России - кавказцы и выходцы из средней Азии, в Великобритании - индийцы, негры, арабы, пакистанцы, etc. А в маленьких, нейтральных странах, вроде Словении или Беларуси, Литвы или Чехии, Румынии или Эстонии - всё пучком. Им вымирание не грозит, они остаются и будут оставаться белыми. Более того, у них ведётся политика, направленная на сохранение традиционных ценностей и культуры коренного населения. Они сказали беженцам нет . В Польшу, например, русскому или украинцу гораздо легче переехать и остаться, чем арабу или африканцу. В Германии ситуация противоположная, белых там не ждут. Польша, Чехия, Словакия, Венгрия, Словения, Хорватия, Сербия, БиГ, Черногория, Македония, Греция, Болгария, Румыния, Молдова, Украина, Беларусь, Литва, Латвия, Эстония - вот Европа будущего. Скандинавия, Южная, Западная Европа, а также Россия - лишатся коренного населения и своей культуры.\"\n",
    "\n",
    "print(\"Пример 1: \", vectorize_w2v(preprocess_text(example_doc_1, True), model_w2v), end=\"\\n\\n\\n\")\n",
    "print(\"Пример 2: \", vectorize_w2v(preprocess_text(example_doc_2, True), model_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a38f81b9629f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(path: str, docs_paths: list[str]):\n",
    "    vectorized_documents = {}\n",
    "\n",
    "    for filename in tqdm(docs_paths):\n",
    "        stems = get_stems_processed(filename)\n",
    "        doc_id = \"/\".join(filename.split(\"/\")[-2:]).replace(\" \", \"\")\n",
    "\n",
    "        vectorized_documents[doc_id] = vectorize_w2v(stems, model_w2v)\n",
    "\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        for k in vectorized_documents.keys():\n",
    "            print(k.replace(\".tsv\", \"\"), *vectorized_documents[k], sep=\"\\t\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae19135",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATASET_EMBEDDINGS_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(DATASET_EMBEDDINGS_TEST_DIR, exist_ok=True)\n",
    "\n",
    "save_embeddings(os.path.join(DATASET_EMBEDDINGS_TRAIN_DIR, \"train_embeddings.tsv\"), docs_file_paths_train)\n",
    "save_embeddings(os.path.join(DATASET_EMBEDDINGS_TEST_DIR, \"test_embeddings.tsv\"), docs_file_paths_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
