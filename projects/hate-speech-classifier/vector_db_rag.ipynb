{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_TO_LOAD = \"/content/drive/MyDrive/itmo_tasks/nlp/assets/data/annotated_tsv/train_test_lists.json\" if IN_COLAB else \"./assets/data/annotaed-tsv/train_test_lists.json\"\n",
    "\n",
    "DB_NAME_PREFIX = \"toxicity\"\n",
    "DB_DIR = \"/content/drive/MyDrive/itmo_tasks/nlp/assets/data/databases/vector_db_toxicity\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DB_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocTokenizer():\n",
    "    def __init__(self, lang: str = \"russian\") -> None:\n",
    "        self.lang = lang\n",
    "\n",
    "    def split_to_sentences(self, text: str) -> list[str]:\n",
    "\n",
    "        sentences = re.split(r\"(!+\\?+)|(\\?+!+)|(\\.{2,})|(\\?{2,})|(!{2,})|(\\? )|(! )|(\\. )\", text)[::9]\n",
    "        return sentences\n",
    "\n",
    "    def split_to_words(self, sentence: str) -> list[str]:\n",
    "        words = re.findall(r\"\\w+@\\w+\\.\\w+|\\+\\d{1,3}-\\d{3}-\\d{3}-\\d{2}-\\d{2}|\\w+\", sentence)\n",
    "        return words\n",
    "\n",
    "    def doc_to_sents(self, text: str) -> list[list[str]]:\n",
    "\n",
    "        sentences = self.split_to_sentences(text)\n",
    "        result = []\n",
    "        for s in sentences:\n",
    "            sentence = []\n",
    "            for w in self.split_to_words(s):\n",
    "                w_processed = re.sub(r\"[.!?,]$\", \"\", w).lower()\n",
    "                sentence.append(w_processed)\n",
    "\n",
    "            result.append(sentence)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocSplitter:\n",
    "    def __init__(self, fragment_size: int = 100, overlap: int = 0, doc_tokenizer = None, token_splitter: str = \" \") -> None:\n",
    "        self.overlap = overlap\n",
    "        self.doc_tokenizer = doc_tokenizer\n",
    "        self.token_splitter = token_splitter\n",
    "        self.fragment_size = fragment_size\n",
    "\n",
    "    def _concat_sents(self, sents: list[list[str]]):\n",
    "        return self.token_splitter.join([self.token_splitter.join(sent) for sent in sents])\n",
    "\n",
    "    def split_doc(self, doc: list[list[str]] | str) -> list[str]:\n",
    "        if isinstance(doc, str):\n",
    "            doc = self.doc_tokenizer.doc_to_sents(doc)\n",
    "\n",
    "\n",
    "        result = []\n",
    "        sent_lens = [len(sent) for sent in doc]\n",
    "        left_sent_id = 0\n",
    "        while left_sent_id < len(sent_lens):\n",
    "            right_sent_id = left_sent_id\n",
    "            curr_frag_size = 0\n",
    "\n",
    "            while curr_frag_size < self.fragment_size and right_sent_id < len(sent_lens):\n",
    "                curr_frag_size += sent_lens[right_sent_id]\n",
    "                right_sent_id += 1\n",
    "\n",
    "            result.append(self._concat_sents(doc[left_sent_id:right_sent_id]))\n",
    "\n",
    "            left_sent_id = right_sent_id\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, paths_to_documents: list[str], doc_splitter: DocSplitter) -> None:\n",
    "        self.paths_to_documents = paths_to_documents\n",
    "        self.doc_splitter = doc_splitter\n",
    "        self.docs = []\n",
    "        self.metas = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_doc_tsv(path: str) -> list[list[str]]:\n",
    "        sentences = []\n",
    "        with open(path, mode=\"r\") as f:\n",
    "            f.readline()\n",
    "            lines = \"\".join(f.readlines())\n",
    "            sentences_raw = lines.split(\"\\n\\t\\t\\n\")\n",
    "            for sentence in sentences_raw:\n",
    "                words = sentence.split(\"\\n\")\n",
    "                if len(words) == 0 or words[0] == \"\":\n",
    "                    continue\n",
    "                tokens = list(map(lambda x: x.split(\"\\t\")[0], words))\n",
    "                sentences.append(tokens)\n",
    "        return sentences\n",
    "\n",
    "    def _load_data_sync(self):\n",
    "        metas = []\n",
    "        for filepath in self.paths_to_documents:\n",
    "            p = Path(filepath)\n",
    "            meta = {\n",
    "                \"document\": p.stem.strip(),\n",
    "                \"topic\": p.parent.stem,\n",
    "            }\n",
    "            metas.append(meta)\n",
    "\n",
    "        doc_sents = [self._load_doc_tsv(filepath) for filepath in tqdm(self.paths_to_documents, desc=\"Loading files\")]\n",
    "\n",
    "        for text, meta in tqdm(zip(doc_sents, metas), desc=\"Creating fragments\"):\n",
    "            frags = self.doc_splitter.split_doc(text)\n",
    "\n",
    "            self.metas.extend([meta] * len(frags))\n",
    "            self.docs.extend(frags)\n",
    "\n",
    "    def _load_data(self, async_=False):\n",
    "        if async_:\n",
    "            # asyncio.run(self._load_data_coro())\n",
    "            pass\n",
    "        else:\n",
    "            self._load_data_sync()\n",
    "\n",
    "    def prefetch_dataset(self):\n",
    "        if not self.docs:\n",
    "            self._load_data()\n",
    "\n",
    "    def reset_dataset(self):\n",
    "        self.docs = []\n",
    "        self.metas = []\n",
    "\n",
    "    def update_dataset(self):\n",
    "        self.reset_dataset()\n",
    "        self.prefetch_dataset()\n",
    "\n",
    "    def get_documents(self, batch_size: int = 1024):\n",
    "        if not self.docs:\n",
    "            self._load_data()\n",
    "\n",
    "        l = 0\n",
    "        while l < len(self.docs):\n",
    "            r = min(l + batch_size, len(self.docs))\n",
    "            yield self.docs[l:r], self.metas[l:r]\n",
    "            l = r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingFunction:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self.model.encode(input).tolist()\n",
    "\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name_prefix, root_path, embeddnig_fn, distance_fn) -> None:\n",
    "        self.client = chromadb.PersistentClient(path=root_path)\n",
    "        self.distance_fn = distance_fn\n",
    "        self.embedding_fn = embeddnig_fn\n",
    "        self._collection_name = name_prefix + self.distance_fn\n",
    "\n",
    "        self._get_or_create_collection()\n",
    "\n",
    "\n",
    "    def _get_or_create_collection(self):\n",
    "        self.database = self.client.get_or_create_collection(\n",
    "            self._collection_name,\n",
    "            metadata={\"hnsw:space\": self.distance_fn},\n",
    "            embedding_function=self.embedding_fn\n",
    "        )\n",
    "\n",
    "    def _delete_collection(self):\n",
    "        self.client.delete_collection(self._collection_name)\n",
    "\n",
    "    def load_dataset(self, dataset: Dataset) -> None:\n",
    "        batch_size = 128\n",
    "        left_i = 0\n",
    "        right_i = 0\n",
    "        for texts, metas in tqdm(dataset.get_documents(batch_size=batch_size), total=math.ceil(len(dataset.docs) / batch_size), desc=\"loading dataset to the DB\"):\n",
    "            right_i = left_i + len(texts)\n",
    "            self.database.add(\n",
    "                documents=texts,\n",
    "                metadatas=metas,\n",
    "                ids=list(map(str, range(left_i, right_i))),\n",
    "                # ids=list([f\"{meta['topic']}/{meta['document']}\" for meta in metas]),\n",
    "            )\n",
    "            left_i = right_i\n",
    "\n",
    "\n",
    "    def query(self, query, n_results: int):\n",
    "        return self.database.query(query_embeddings=self.embedding_fn(query), n_results=n_results)\n",
    "\n",
    "    def clear(self):\n",
    "        self._delete_collection()\n",
    "        self._get_or_create_collection()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DOCS_TO_LOAD) as f:\n",
    "    dataset_meta = json.load(f)\n",
    "\n",
    "docs_paths = dataset_meta[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = DocSplitter(doc_tokenizer=DocTokenizer())\n",
    "dataset = Dataset(docs_paths, splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_cos = VectorDB(\"toxicity\", DB_DIR, EmbeddingFunction(), \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_l2 = VectorDB(\"toxicity\", DB_DIR, EmbeddingFunction(), \"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ip = VectorDB(\"toxicity\", DB_DIR, EmbeddingFunction(), \"ip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.update_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_cos.load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_l2.load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ip.load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DOCS_TO_LOAD) as f:\n",
    "    dataset_meta = json.load(f)\n",
    "\n",
    "# docs for the db\n",
    "train_docs_paths = dataset_meta[\"train\"]\n",
    "\n",
    "dataset_train = Dataset(train_docs_paths, splitter)\n",
    "dataset_train.update_dataset()\n",
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_cos.load_dataset(dataset_train)\n",
    "database_l2.load_dataset(dataset_train)\n",
    "database_ip.load_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_to_test = (\n",
    "    \"Зачем ты что-то пишешь потому что могу у тебя бамбануло, хаха порвался с учётом ваших длинных комментариев- так и есть смеешься там как умственно отсталый Это как? Есть какие-либо критерии как смеются умственно отсталые? Или ты как обычно пишешь абстрактные вещи, смысл которые даже тебе не понятен?\",\n",
    "    \"Нашёл как то работу отличную с зп в районе 50-60к, для меня это было просто супер, тк на старой я получал 15-30, обязанности все те же и условия лучше, собеседование и стажировку в 3 дня прошёл успешно я им понравился, но не пропустил сб, хотя до этого работы другой не было, а на этой косяков не было)\",\n",
    "    \"В 2021 году астрономы добавили к списку потенциально обитаемых классов планет ещё один — так называемые гикеановские экзопланеты. Это название — производное от hydrogen (водород) и ocean (океан). По словам учёных, такие планеты — горячие, они полностью покрыты водой, а их атмосфера богата водородом. Одна из них — K2-18b, и о ней сегодня поговорим под катом.\",\n",
    "    \"Similar to diffusion models, consistency models enable various data editing and manipulation applications in zero shot; they do not require explicit training to perform these tasks. For example, consistency models define a one-to-one mapping from a Gaussian noise vector to a data sample.\",\n",
    "    \"Пилат поднял мученические глаза на арестанта и увидел, что солнце уже довольно высоко стоит над гипподромом, что луч пробрался в колоннаду и подползает к стоптанным сандалиям Иешуа, что тот сторонится от солнца.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_samples(samples, db: VectorDB, k=10):\n",
    "    for sample in samples:\n",
    "        results = db.query(sample, k)\n",
    "        print(\"Sample is: \", sample[:20])\n",
    "        print(\"Response is: \")\n",
    "        pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST COSINE SIMILARITY\")\n",
    "query_samples(phrases_to_test, database_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST L2 SIMILARITY\")\n",
    "query_samples(phrases_to_test, database_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST IP SIMILARITY\")\n",
    "query_samples(phrases_to_test, database_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_handler(model, vec_db):\n",
    "    def rag_handler(msg, his):\n",
    "        ctx = \"\\n\".join(vec_db.query(msg, 1)[\"documents\"][0])\n",
    "        print(ctx)\n",
    "        prompt = \"\"\"Ответь на вопрос (Question), учитывая контекст (Context). Длина твоего ответа (Answer) не должна превышать 50 слов. Ответ должен быть на русском языке.\n",
    "\n",
    "        Context: {ctx}\n",
    "\n",
    "        Question: {qst}\n",
    "\n",
    "        Answer: \"\"\".format(ctx=ctx, qst=msg)\n",
    "\n",
    "        answ = model(prompt)\n",
    "        return answ\n",
    "\n",
    "    return rag_handler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-OpenOrca-GGUF\", model_file=\"mistral-7b-openorca.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.ChatInterface(fn=get_rag_handler(model, database_cos), title=\"Чат\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
