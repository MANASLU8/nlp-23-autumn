{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_RAW_PATH = \"./assets/data/raw-data/labeled.csv\"\n",
    "DATASET_DIR_ANNOTATEO_PATH = \"./assets/data/annotaed-tsv/\"\n",
    "LABEL_X = \"comment\"\n",
    "LABEL_Y = \"toxic\"\n",
    "LOG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_documents(collection: pd.DataFrame | pd.Series) -> pd.DataFrame | pd.Series:\n",
    "    collection.replace(r\"([^ А-ЯЁа-яё])|(\\d)\", \" \", regex=True, inplace=True)\n",
    "    collection.replace(r\"\\s{2,}\",\" \", regex=True, inplace=True)\n",
    "    collection.replace(r\"(^ )|( $)\",\"\", regex=True, inplace=True)\n",
    "    return collection\n",
    "\n",
    "\n",
    "stopwords_list = stopwords.words(\"russian\")\n",
    "morph_analyzer = MorphAnalyzer()\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "\n",
    "def preprocess_document(phrase: str, do: typing.Callable) -> str:\n",
    "    phrase = \" \".join([\n",
    "        do(word)\n",
    "        for word in phrase.split()\n",
    "        if phrase not in stopwords_list\n",
    "    ])\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def preprocess_documents(documents: str | typing.Iterable[str], do: typing.Callable) -> typing.Iterable[str]:\n",
    "    if isinstance(documents, str):\n",
    "        ret = preprocess_document(documents, do)\n",
    "\n",
    "    elif isinstance(documents, typing.Iterable):\n",
    "        for i in range(len(documents)):\n",
    "            documents[i] = preprocess_document(documents[i], do)\n",
    "        ret = documents\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"documents must be str or Iterable[str]\")\n",
    "\n",
    "    return ret\n",
    "\n",
    "def norm_documents(documents: str | typing.Iterable[str], *, morph_analyzer=morph_analyzer) -> typing.Iterable[str]:\n",
    "    def do(word: str) -> str:\n",
    "        return morph_analyzer.normal_forms(word)[0]\n",
    "    \n",
    "    return preprocess_documents(documents, do)\n",
    "\n",
    "\n",
    "def stem_documents(documents: str | typing.Iterable[str], *, stemmer=stemmer) -> typing.Iterable[str]:\n",
    "    def do(word: str) -> str:\n",
    "        return stemmer.stem(word)\n",
    "    \n",
    "    return preprocess_documents(documents, do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str, extract_method: typing.Callable = pd.read_csv, **pandas_kwargs: dict[str, any]) -> typing.Collection:\n",
    "    df = extract_method(data_path, **pandas_kwargs)\n",
    "    if LOG:\n",
    "        print(\"Data loaded!   Shape is:\", df.shape)\n",
    "        print(df.head(), \"\\n\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_annotation_dirs(root_path: str, labels: typing.Collection[str]) -> None:\n",
    "    root = PurePath(root_path)\n",
    "    for c in labels:\n",
    "        Path(root.joinpath(c)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_document_annotation(root_path: str, documents: typing.Iterable[typing.Iterable[str]] | pd.DataFrame, class_label: str, doc_id: int) -> None:\n",
    "    root = PurePath(root_path)\n",
    "    file_path = root.joinpath(class_label).joinpath(\"{:6d}.tsv\".format(doc_id))\n",
    "    documents.to_csv(file_path, sep=\"\\t\", index=False)\n",
    "    \n",
    "    if LOG:\n",
    "        print(\"Document with id\", doc_id, \"has saved\")\n",
    "\n",
    "def make_annotations_dataframe(document: str, token_extractor_pattern: re.Pattern, lemmatize: typing.Callable[[str], str], stemmatize: typing.Callable[[str], str], stopwords_list: typing.Collection[str] = stopwords) -> pd.DataFrame:    \n",
    "    words = []\n",
    "\n",
    "    EMPTY_LINE = (\"\", \"\", \"\")\n",
    "\n",
    "    sentence_end_ch = [\".\", \"?\", \"!\", \"\\n\"]\n",
    "    endline_ch = [\"\\n\"]\n",
    "    is_last_symb_sentence_end = False\n",
    "    for token in token_extractor_pattern.finditer(document):\n",
    "        token = token.group().strip(\" \")\n",
    "        \n",
    "        if token and token not in stopwords_list:          \n",
    "            if token[-1] in sentence_end_ch:\n",
    "                is_last_symb_sentence_end = True\n",
    "            elif is_last_symb_sentence_end:\n",
    "                is_last_symb_sentence_end = False\n",
    "                words.append(EMPTY_LINE)\n",
    "            \n",
    "            if token not in endline_ch:\n",
    "                words.append((token, stemmatize(token), lemmatize(token)))\n",
    "\n",
    "\n",
    "    if words[-1] != EMPTY_LINE:\n",
    "        words.append(EMPTY_LINE)\n",
    "    \n",
    "    ret = pd.DataFrame(data=words, columns=[\"token\", \"stem\", \"lemma\"])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def process_data_annotation_pipeline(data: pd.DataFrame, root_path: str, labels: list[str] = [\"non-toxic\", \"toxic\"], language: str = \"russian\") -> None:\n",
    "    stopwords_list = stopwords.words(language)\n",
    "    morph_analyzer = MorphAnalyzer()\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    lemmatize = lambda token: morph_analyzer.normal_forms(token)[0]\n",
    "    stemmatize = lambda token: stemmer.stem(token)\n",
    "\n",
    "    phone_number_regex = r\"(\\+\\d{1,3})?\\s?\\(?\\d{1,4}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\"\n",
    "    email_regex = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\"\n",
    "    word_regex = r\"([А-Яа-яЁёA-Za-z0-9-]+)\"\n",
    "    sent_end_combinations = r\"(!+\\?+)|(\\?+!+)|(\\.{2,})|(\\?{2,})|(!{2,})\"\n",
    "    punct = r\"([,\\.;:\\?!\\n-])\"\n",
    "\n",
    "    token_pattern = re.compile(\"|\".join([\n",
    "        email_regex,\n",
    "        phone_number_regex,\n",
    "        word_regex,\n",
    "        sent_end_combinations,\n",
    "        punct,\n",
    "    ]))\n",
    "\n",
    "    prepare_annotation_dirs(root_path, labels)\n",
    "\n",
    "    for idx, doc in data.iterrows():\n",
    "        annotation_table = make_annotations_dataframe(doc[LABEL_X], token_extractor_pattern=token_pattern, lemmatize=lemmatize, stemmatize=stemmatize, stopwords_list=stopwords_list)\n",
    "        save_document_annotation(root_path, annotation_table, labels[int(doc[LABEL_Y])], idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents = load_data(DATASET_RAW_PATH)\n",
    "process_data_annotation_pipeline(text_documents, DATASET_DIR_ANNOTATEO_PATH)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
